{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Minimal Stationary **HJB–FP × PINN** (JAX + Equinox)\n",
    "\n",
    "Compact, single-notebook implementation of a stationary coupled **Hamilton–Jacobi–Bellman (HJB)** and **Fokker–Planck (FP)**\n",
    "fixed point on a rectangle. Two tiny MLPs (value `V(k,z)` and density `μ(k,z)`) trained by residual losses.\n",
    "**Reflecting (no-flux) boundaries** are penalized via **probability currents**. **Price clears** via a damped, log-space update.\n",
    "\n",
    "*Backbone:* Achdou–Han–Lasry–Lions–Moll (REStud, 2022).  \n",
    "*Irreversibility wedge:* Abel & Eberly (REStud, 1996).  \n",
    "*PINNs & DGM:* Raissi et al. (JCP, 2019); Sirignano & Spiliopoulos (JCP, 2018).  \n",
    "*Neural FP positivity:* Zhai–Dobson–Li (PMLR, 2022).  \n",
    "*No‑flux boundaries:* standard FP current conditions.\n",
    "\n",
    "**Non-goals:** No time dependence, no grids/FEM, no fancy samplers. Keep it tiny.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure deps (CPU) are available; install if missing\n",
    "try:\n",
    "    import jax, equinox, optax, sympy, matplotlib  # noqa: F401\n",
    "except Exception:  # pragma: no cover\n",
    "    import sys, subprocess\n",
    "    pkgs = [\"jax\", \"equinox>=0.11\", \"optax>=0.2\", \"sympy\", \"matplotlib\"]\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\", *pkgs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, functools\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit, vmap, jacrev, hessian, random\n",
    "\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "\n",
    "import sympy as sp\n",
    "\n",
    "# Use float64 for stability (must be set at startup)\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "key = random.PRNGKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    # Domain\n",
    "    k_min: float = 0.2\n",
    "    k_max: float = 5.0\n",
    "    z_max: float = 2.5\n",
    "\n",
    "    # Technology / preferences (stylized)\n",
    "    alpha: float = 0.35\n",
    "    delta: float = 0.08\n",
    "    r: float = 0.05\n",
    "\n",
    "    # OU shock in z\n",
    "    kappa_z: float = 0.5\n",
    "    sigma_z: float = 0.2\n",
    "\n",
    "    # Smooth costly (ir)reversibility\n",
    "    theta_plus: float = 2.0\n",
    "    theta_minus: float = 6.0\n",
    "    beta_theta: float = 5.0\n",
    "\n",
    "    # Price aggregator\n",
    "    eta: float = 1.0         # P = Y^{-eta}\n",
    "    x_log: float = 0.0       # y = exp(x + z) k^alpha\n",
    "    lam_price: float = 0.7   # damping in log space\n",
    "\n",
    "    # Training sizes\n",
    "    n_colloc: int = 2000\n",
    "    n_bndry: int = 1000     # total; split across edges\n",
    "    n_mass: int  = 4000\n",
    "    n_agg: int   = 4000\n",
    "\n",
    "    # Optimization\n",
    "    steps_V: int = 500\n",
    "    steps_M: int = 500\n",
    "    outer_steps: int = 8\n",
    "    lr_V: float = 3e-3\n",
    "    lr_M: float = 3e-3\n",
    "    grad_clip: float = 1.0\n",
    "\n",
    "    # Loss weights\n",
    "    w_bndry: float = 1.0\n",
    "    w_mass: float = 1.0\n",
    "\n",
    "    # Early stop on price\n",
    "    tol_logP: float = 5e-4\n",
    "\n",
    "par = Params()\n",
    "area = (par.k_max - par.k_min) * (2.0 * par.z_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AffineBox(eqx.Module):\n",
    "    k_min: float\n",
    "    k_max: float\n",
    "    z_max: float\n",
    "\n",
    "    def __call__(self, k: jnp.ndarray, z: jnp.ndarray):\n",
    "        # map box -> [-1,1]^2\n",
    "        khat = 2.0 * (k - self.k_min) / (self.k_max - self.k_min) - 1.0\n",
    "        zhat = z / self.z_max  # since z \\in [-zmax,zmax]\n",
    "        return jnp.stack([khat, zhat], axis=-1)\n",
    "\n",
    "class ValueNet(eqx.Module):\n",
    "    box: AffineBox\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, key, width=64, depth=3):\n",
    "        box = AffineBox(par.k_min, par.k_max, par.z_max)\n",
    "        mlp = eqx.nn.MLP(\n",
    "            in_size=2, out_size=1, width_size=width, depth=depth,\n",
    "            key=key, activation=jnp.tanh, final_activation=lambda y: y,\n",
    "        )\n",
    "        self.box, self.mlp = box, mlp\n",
    "\n",
    "    def __call__(self, k: jnp.ndarray, z: jnp.ndarray):\n",
    "        x = self.box(k, z)                 # inside map; autodiff sees physical inputs\n",
    "        return self.mlp(x).squeeze(-1)     # scalar V\n",
    "\n",
    "class DensityNet(eqx.Module):\n",
    "    box: AffineBox\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, key, width=64, depth=3):\n",
    "        box = AffineBox(par.k_min, par.k_max, par.z_max)\n",
    "        mlp = eqx.nn.MLP(\n",
    "            in_size=2, out_size=1, width_size=width, depth=depth,\n",
    "            key=key, activation=jnp.tanh, final_activation=lambda y: y,\n",
    "        )\n",
    "        self.box, self.mlp = box, mlp\n",
    "\n",
    "    def __call__(self, k: jnp.ndarray, z: jnp.ndarray):\n",
    "        x = self.box(k, z)\n",
    "        s = self.mlp(x).squeeze(-1)\n",
    "        # positivity for μ\n",
    "        return jax.nn.softplus(s) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sample_interior(key, n):\n",
    "    k = random.uniform(key, (n,), minval=par.k_min, maxval=par.k_max)\n",
    "    z = random.uniform(key, (n,), minval=-par.z_max, maxval=par.z_max)\n",
    "    return k, z\n",
    "\n",
    "def sample_k_edges(key, n_each):\n",
    "    # k = k_min or k_max; z uniform\n",
    "    z = random.uniform(key, (2*n_each,), minval=-par.z_max, maxval=par.z_max)\n",
    "    k_edge = jnp.concatenate([jnp.full((n_each,), par.k_min),\n",
    "                              jnp.full((n_each,), par.k_max)])\n",
    "    return k_edge, z\n",
    "\n",
    "def sample_z_edges(key, n_each):\n",
    "    # z = ± z_max; k uniform\n",
    "    k = random.uniform(key, (2*n_each,), minval=par.k_min, maxval=par.k_max)\n",
    "    z_edge = jnp.concatenate([jnp.full((n_each,), -par.z_max),\n",
    "                              jnp.full((n_each,),  par.z_max)])\n",
    "    return k, z_edge\n",
    "\n",
    "# Vectorized nets\n",
    "def V_eval(Vnet, k, z):\n",
    "    return vmap(Vnet)(k, z)\n",
    "\n",
    "def mu_eval(Mnet, k, z):\n",
    "    return vmap(Mnet)(k, z)\n",
    "\n",
    "# Gradients/Hessians in physical coordinates (chain handled by inside map)\n",
    "def V_grads(Vnet, k, z):\n",
    "    def g(kz):\n",
    "        return Vnet(kz[0], kz[1])\n",
    "    X = jnp.stack([k, z], axis=1)\n",
    "    dV = vmap(jacrev(g))(X)            # (n,2)\n",
    "    H  = vmap(hessian(g))(X)           # (n,2,2)\n",
    "    Vv = vmap(g)(X)                    # (n,)\n",
    "    return Vv, dV[:,0], dV[:,1], H[:,1,1]\n",
    "\n",
    "def mu_grads(Mnet, k, z):\n",
    "    def m(kz):\n",
    "        return Mnet(kz[0], kz[1])\n",
    "    X = jnp.stack([k, z], axis=1)\n",
    "    dmu = vmap(jacrev(m))(X)           # (n,2)\n",
    "    H   = vmap(hessian(m))(X)          # (n,2,2)\n",
    "    muv = vmap(m)(X)\n",
    "    return muv, dmu[:,0], dmu[:,1], H[:,1,1]  # μ, μ_k, μ_z, μ_zz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def theta_of(V_k):\n",
    "    # θ(V_k) = θ_+ + (θ_- - θ_+) σ(-β(V_k-1))\n",
    "    return par.theta_plus + (par.theta_minus - par.theta_plus) * jax.nn.sigmoid(-par.beta_theta * (V_k - 1.0))\n",
    "\n",
    "def inv_policy(V_k, k):\n",
    "    th = theta_of(V_k)\n",
    "    return (k / th) * (V_k - 1.0)      # i^*\n",
    "\n",
    "def prod_y(k, z):\n",
    "    return jnp.exp(par.x_log + z) * (k ** par.alpha)\n",
    "\n",
    "def hjb_residual(Vnet, Mnet, P, k, z):\n",
    "    V, V_k, V_z, V_zz = V_grads(Vnet, k, z)\n",
    "    i_star = inv_policy(V_k, k)\n",
    "    theta  = theta_of(V_k)\n",
    "\n",
    "    term_profit = P * prod_y(k, z)\n",
    "    term_k      = - par.delta * k * V_k\n",
    "    term_cost   = (k / (2.0 * theta)) * (V_k - 1.0)**2\n",
    "    term_zdiff  = 0.5 * (par.sigma_z**2) * V_zz\n",
    "    term_zdrift = - par.kappa_z * z * V_z\n",
    "\n",
    "    H = term_profit + term_k + term_cost + term_zdiff + term_zdrift\n",
    "    return par.r * V - H  # residual; target 0\n",
    "\n",
    "def fp_residual(Vnet, Mnet, k, z):\n",
    "    # Build probability current J = (J_k, J_z) and take divergence\n",
    "    # J_k = (i* - δk) μ\n",
    "    # J_z = (-κ z) μ - (σ^2/2) ∂_z μ\n",
    "    V, V_k, V_z, V_zz = V_grads(Vnet, k, z)\n",
    "    mu, mu_k, mu_z, mu_zz = mu_grads(Mnet, k, z)\n",
    "    i_star = inv_policy(V_k, k)\n",
    "\n",
    "    def J_single(kz):\n",
    "        k_, z_ = kz[0], kz[1]\n",
    "        # recompute scalars at (k_,z_)\n",
    "        V_, V_k_, _, _ = V_grads(Vnet, jnp.array([k_]), jnp.array([z_]))\n",
    "        mu_, _, mu_z_, _ = mu_grads(Mnet, jnp.array([k_]), jnp.array([z_]))\n",
    "        V_k_ = V_k_.squeeze()\n",
    "        mu_  = mu_.squeeze()\n",
    "        mu_z_= mu_z_.squeeze()\n",
    "        i_   = inv_policy(V_k_, k_)\n",
    "        Jk = (i_ - par.delta * k_) * mu_\n",
    "        Jz = (-par.kappa_z * z_) * mu_ - 0.5 * (par.sigma_z**2) * mu_z_\n",
    "        return jnp.stack([Jk, Jz])\n",
    "\n",
    "    X = jnp.stack([k, z], axis=1)\n",
    "    # Jacobian of J wrt (k,z): (n,2,2)\n",
    "    dJ = vmap(jacrev(J_single))(X)\n",
    "    divJ = dJ[:,0,0] + dJ[:,1,1]\n",
    "    # Stationary FP: divJ = 0  (since ∂_t μ = -divJ)\n",
    "    return divJ\n",
    "\n",
    "def boundary_currents(Vnet, Mnet, k_edges, z_for_k, k_for_z, z_edges):\n",
    "    # j_k = (i* - δk) μ on k-edges\n",
    "    V, V_k, _, _ = V_grads(Vnet, k_edges, z_for_k)\n",
    "    mu, _, _, _ = mu_grads(Mnet, k_edges, z_for_k)\n",
    "    i_star = inv_policy(V_k, k_edges)\n",
    "    j_k = (i_star - par.delta * k_edges) * mu\n",
    "\n",
    "    # j_z = (-κ z) μ - (σ^2/2) ∂_z μ on z-edges\n",
    "    mu2, _, mu_z, _ = mu_grads(Mnet, k_for_z, z_edges)\n",
    "    j_z = (-par.kappa_z * z_edges) * mu2 - 0.5 * (par.sigma_z**2) * mu_z\n",
    "\n",
    "    return j_k, j_z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def losses(Vnet, Mnet, P, key):\n",
    "    # Interior samples\n",
    "    key_int, key_bk, key_bz, key_mass = random.split(key, 4)\n",
    "    k_i, z_i = sample_interior(key_int, par.n_colloc)\n",
    "\n",
    "    # Residuals\n",
    "    R_hjb = hjb_residual(Vnet, Mnet, P, k_i, z_i)\n",
    "    R_fp  = fp_residual(Vnet, Mnet, k_i, z_i)\n",
    "\n",
    "    L_hjb = jnp.mean(R_hjb**2)\n",
    "    L_fp  = jnp.mean(R_fp**2)\n",
    "\n",
    "    # Boundary penalties\n",
    "    n_each = par.n_bndry // 4\n",
    "    k_edge, z_for_k = sample_k_edges(key_bk, n_each)\n",
    "    k_for_z, z_edge = sample_z_edges(key_bz, n_each)\n",
    "\n",
    "    j_k, j_z = boundary_currents(Vnet, Mnet, k_edge, z_for_k, k_for_z, z_edge)\n",
    "    L_bndry = jnp.mean(j_k**2) + jnp.mean(j_z**2)\n",
    "\n",
    "    # Mass penalty\n",
    "    k_m, z_m = sample_interior(key_mass, par.n_mass)\n",
    "    mu_m = mu_eval(Mnet, k_m, z_m)\n",
    "    mass_hat = area * jnp.mean(mu_m)\n",
    "    L_mass = (mass_hat - 1.0)**2\n",
    "\n",
    "    return L_hjb, L_fp, L_bndry, L_mass, mass_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_Y(Mnet, key):\n",
    "    k_a, z_a = sample_interior(key, par.n_agg)\n",
    "    mu_a = mu_eval(Mnet, k_a, z_a)\n",
    "    y_a  = prod_y(k_a, z_a)\n",
    "    # self-normalized MC: Y = (∫ y μ)/(∫ μ)\n",
    "    num = jnp.mean(y_a * mu_a)\n",
    "    den = jnp.mean(mu_a) + 1e-12\n",
    "    return (num / den)\n",
    "\n",
    "def update_price(P, Y):\n",
    "    logP = jnp.log(P + 1e-12)\n",
    "    logP_target = -par.eta * jnp.log(Y + 1e-12)\n",
    "    logP_new = (1.0 - par.lam_price) * logP + par.lam_price * logP_target\n",
    "    return jnp.exp(logP_new), (logP_new - logP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize nets\n",
    "k1, k2 = random.split(key, 2)\n",
    "Vnet = ValueNet(k1, width=64, depth=3)\n",
    "Mnet = DensityNet(k2, width=64, depth=3)\n",
    "\n",
    "# Optax optimizers\n",
    "optV = optax.chain(optax.clip_by_global_norm(par.grad_clip), optax.adam(par.lr_V))\n",
    "optM = optax.chain(optax.clip_by_global_norm(par.grad_clip), optax.adam(par.lr_M))\n",
    "optV_state = optV.init(eqx.filter(Vnet, eqx.is_array))\n",
    "optM_state = optM.init(eqx.filter(Mnet, eqx.is_array))\n",
    "\n",
    "P = jnp.array(1.0)  # initial price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@eqx.filter_jit\n",
    "def step_V(Vnet, Mnet, P, opt_state, key):\n",
    "    L_hjb, L_fp, L_bndry, L_mass, mass_hat = losses(Vnet, Mnet, P, key)\n",
    "    def loss_only_V(Vnet2):\n",
    "        R = hjb_residual(Vnet2, Mnet, P, *sample_interior(key, par.n_colloc))\n",
    "        return jnp.mean(R**2)\n",
    "    grads = eqx.filter_grad(loss_only_V)(Vnet)\n",
    "    updates, new_state = optV.update(eqx.filter(grads, eqx.is_array), opt_state, eqx.filter(Vnet, eqx.is_array))\n",
    "    Vnet = eqx.apply_updates(Vnet, updates)\n",
    "    return Vnet, new_state, dict(L_HJB=L_hjb, L_FP=L_fp, L_B=L_bndry, L_M=L_mass, mass=mass_hat)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def step_M(Vnet, Mnet, P, opt_state, key):\n",
    "    def loss_M(Mnet2):\n",
    "        L_hjb, L_fp, L_bndry, L_mass, mass_hat = losses(Vnet, Mnet2, P, key)\n",
    "        return L_fp + par.w_bndry * L_bndry + par.w_mass * L_mass, (L_hjb, L_fp, L_bndry, L_mass, mass_hat)\n",
    "    (L_total, aux), grads = eqx.filter_value_and_grad(loss_M, has_aux=True)(Mnet)\n",
    "    updates, new_state = optM.update(eqx.filter(grads, eqx.is_array), opt_state, eqx.filter(Mnet, eqx.is_array))\n",
    "    Mnet = eqx.apply_updates(Mnet, updates)\n",
    "    L_hjb, L_fp, L_bndry, L_mass, mass_hat = aux\n",
    "    return Mnet, new_state, dict(L_HJB=L_hjb, L_FP=L_fp, L_B=L_bndry, L_M=L_mass, mass=mass_hat, L_total=L_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "[outer 1] L_HJB=1.001e-01 L_FP=2.921e-03 L_B=1.030e-02 L_M=1.600e-03 mass=0.9600 Y=0.4580 P=1.0000 ΔlogP=+5.47e-01\n",
      "[outer 2] L_HJB=1.504e-02 L_FP=2.582e-03 L_B=6.998e-03 L_M=3.954e-03 mass=1.0629 Y=0.1314 P=1.7275 ΔlogP=+1.04e+00\n",
      "[outer 3] L_HJB=1.031e+02 L_FP=2.357e-03 L_B=7.993e-03 L_M=2.328e-02 mass=0.8474 Y=0.0706 P=4.8779 ΔlogP=+7.47e-01\n",
      "[outer 4] L_HJB=1.072e+03 L_FP=1.020e-01 L_B=3.096e-02 L_M=6.290e-03 mass=1.0793 Y=0.0597 P=10.2925 ΔlogP=+3.40e-01\n",
      "[outer 5] L_HJB=2.753e+03 L_FP=1.639e-01 L_B=9.068e-03 L_M=4.306e-01 mass=0.3438 Y=0.0530 P=14.4676 ΔlogP=+1.86e-01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Training...\")\n",
    "key_loop = key\n",
    "prev_logP = jnp.log(P + 1e-12)\n",
    "\n",
    "for outer in range(par.outer_steps):\n",
    "    # value steps\n",
    "    for t in range(par.steps_V):\n",
    "        key_loop, sub = random.split(key_loop)\n",
    "        Vnet, optV_state, diagV = step_V(Vnet, Mnet, P, optV_state, sub)\n",
    "\n",
    "    # density steps\n",
    "    for t in range(par.steps_M):\n",
    "        key_loop, sub = random.split(key_loop)\n",
    "        Mnet, optM_state, diagM = step_M(Vnet, Mnet, P, optM_state, sub)\n",
    "\n",
    "    # aggregate and update price\n",
    "    key_loop, sub = random.split(key_loop)\n",
    "    Y = aggregate_Y(Mnet, sub)\n",
    "    P_new, dlogP = update_price(P, Y)\n",
    "\n",
    "    # Diagnostics\n",
    "    msg = (f\"[outer {outer+1}] \"\n",
    "           f\"L_HJB={float(diagM['L_HJB']):.3e} \"\n",
    "           f\"L_FP={float(diagM['L_FP']):.3e} \"\n",
    "           f\"L_B={float(diagM['L_B']):.3e} \"\n",
    "           f\"L_M={float(diagM['L_M']):.3e} \"\n",
    "           f\"mass={float(diagM['mass']):.4f} \"\n",
    "           f\"Y={float(Y):.4f} P={float(P):.4f} ΔlogP={float(dlogP):+.2e}\")\n",
    "    print(msg)\n",
    "\n",
    "    # early stop\n",
    "    if jnp.abs(dlogP) < par.tol_logP:\n",
    "        P = P_new\n",
    "        print(\"Early stop: price stabilized.\")\n",
    "        break\n",
    "\n",
    "    P, prev_logP = P_new, prev_logP + dlogP\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify the simple conjugacy when θ is constant:\n",
    "Vk, i, th, k_sym = sp.symbols('V_k i theta k', positive=True)\n",
    "H = (Vk - 1)*i - (th/2) * i**2 / k_sym\n",
    "dHdi = sp.diff(H, i)\n",
    "i_star_sym = sp.solve(sp.Eq(dHdi, 0), i)[0]\n",
    "H_star = sp.simplify(H.subs(i, i_star_sym))\n",
    "sp.simplify(i_star_sym), sp.simplify(H_star)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Optional: quick slice plot of μ(k, z=0) after training\n",
    "ks = jnp.linspace(par.k_min, par.k_max, 200)\n",
    "zs = jnp.zeros_like(ks)\n",
    "mu_slice = mu_eval(Mnet, ks, zs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ks, mu_slice)\n",
    "plt.xlabel(\"k (capital)\")\n",
    "plt.ylabel(\"μ(k, z=0)\")\n",
    "plt.title(\"Density slice after training\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final diagnostics\n",
    "k_a, z_a = sample_interior(random.PRNGKey(123), par.n_agg)\n",
    "mu_a = mu_eval(Mnet, k_a, z_a)\n",
    "mass_hat = area * jnp.mean(mu_a)\n",
    "Y = aggregate_Y(Mnet, random.PRNGKey(456))\n",
    "print(f\"Final mass ≈ {float(mass_hat):.6f},  Final Y ≈ {float(Y):.6f},  Final P ≈ {float(P):.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
