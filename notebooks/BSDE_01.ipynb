{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fac3fd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two Trees (log-utility) via Deep-FBSDE, with Heun–Stratonovich BSDE loss\n",
    "# Math anchors: Eq. (21)–(23), (48), (50) in Two Trees; BSDE↔PDE (Pardoux–Peng).\n",
    "# Citations in accompanying notes.\n",
    "\n",
    "import jax, jax.numpy as jnp\n",
    "import equinox as eqx, optax\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# -------------------- Model parameters and share SDE --------------------\n",
    "class Params(eqx.Module):\n",
    "    delta: float\n",
    "    mu1: float; mu2: float\n",
    "    sig1: float; sig2: float\n",
    "    rho: float\n",
    "\n",
    "def eta2(p):  # Var of σ1 dZ1 − σ2 dZ2 with corr ρ\n",
    "    return p.sig1**2 + p.sig2**2 - 2.0*p.rho*p.sig1*p.sig2\n",
    "\n",
    "def b_ito(s, p):\n",
    "    # Itô drift from Eq. (48)\n",
    "    return s*(1.0 - s)*(p.mu1 - p.mu2 - s*p.sig1**2 + (1.0 - s)*p.sig2**2\n",
    "                        + 2.0*(s - 0.5)*p.rho*p.sig1*p.sig2)\n",
    "\n",
    "def sig_s(s, p):                  # scalar diffusion magnitude\n",
    "    return s*(1.0 - s)*jnp.sqrt(eta2(p))\n",
    "\n",
    "def b_strat(s, p):               # Itô → Stratonovich: b_S = b_I - 0.5 * Σ G ∂G\n",
    "    # Here G(s) = s(1-s)[σ1, -σ2] with corr matrix [[1,ρ],[ρ,1]] ⇒ correction = 0.5*η²*s(1-s)(1-2s)\n",
    "    e2 = eta2(p)\n",
    "    return b_ito(s, p) - 0.5 * e2 * s*(1.0 - 1.0*s)*(1.0 - 2.0*s)\n",
    "\n",
    "\n",
    "def heun_strat_step(s, dt, dW, p):\n",
    "    s = jnp.clip(s, 1e-8, 1.0 - 1e-8)\n",
    "    bS = b_strat(s, p);  sig = sig_s(s, p)\n",
    "    s_pred = jnp.clip(s + bS*dt + sig*dW, 1e-8, 1.0 - 1e-8)\n",
    "    return jnp.clip(s + 0.5*(bS + b_strat(s_pred, p))*dt\n",
    "                      + 0.5*(sig + sig_s(s_pred, p))*dW, 1e-8, 1.0 - 1.0*1e-8)\n",
    "\n",
    "# -------------------- Single head with hard boundary conditions --------------------\n",
    "class Head(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    def __init__(self, key, width=32, depth=2):\n",
    "        self.mlp = eqx.nn.MLP(1, 1, width, depth, key=key, activation=jax.nn.tanh)\n",
    "    def __call__(self, s, delta):\n",
    "        # Enforces Y(0)=0 and Y(1)=1/delta\n",
    "        h = self.mlp(s[..., None]).squeeze(-1)\n",
    "        return (s + s*(1.0 - s)*h) / delta\n",
    "\n",
    "# -------------------- Path simulation (antithetic) --------------------\n",
    "def sim_paths(key, p, n_paths=4096, n_steps=64, T=1.0):\n",
    "    dt = T / n_steps\n",
    "    k = jax.random.split(key, 1)[0]\n",
    "    dW = jax.random.normal(k, (n_paths, n_steps)) * jnp.sqrt(dt)\n",
    "    dW = jnp.concatenate([dW, -dW], axis=0)  # antithetic → shape (2*n_paths, n_steps)\n",
    "    s0 = jnp.full((dW.shape[0],), 0.5)\n",
    "    def step(carry, dw):\n",
    "        new_s = heun_strat_step(carry, dt, dw, p)\n",
    "        return new_s, new_s  # emit state to collect full path\n",
    "    last_s, s_hist = jax.lax.scan(step, s0, dW.T)   # s_hist: [steps, paths]\n",
    "    S = s_hist.T                                    # [paths, steps]\n",
    "    return S, dW, dt\n",
    "\n",
    "# Helpers that robustly handle 0D/1D/2D inputs by vectorizing along axis 0 when present\n",
    "\n",
    "def _flatten_vmap_reshape(fun, X):\n",
    "    \"\"\"Apply fun elementwise preserving (B,T) layout; supports 0D/1D/2D.\n",
    "    fun: maps scalar→scalar. X: () | (B,) | (B,T). Returns same shape as X.\n",
    "    \"\"\"\n",
    "    ndim = jnp.ndim(X)\n",
    "    if ndim == 0:\n",
    "        return fun(X)\n",
    "    if ndim == 1:\n",
    "        return jax.vmap(fun)(X)\n",
    "    if ndim == 2:\n",
    "        B, T = X.shape\n",
    "        Y = jax.vmap(fun)(X.reshape(B*T,))\n",
    "        return Y.reshape(B, T)\n",
    "    raise ValueError(f\"_flatten_vmap_reshape: unsupported ndim {ndim} for shape {jnp.shape(X)}\")\n",
    "\n",
    "def f_apply(model, p, x):\n",
    "    f = lambda z: model(z, p.delta)\n",
    "    return _flatten_vmap_reshape(f, x)\n",
    "\n",
    "def fprime_batch(model, p, X):\n",
    "    g = jax.grad(lambda z: model(z, p.delta))\n",
    "    return _flatten_vmap_reshape(g, X)\n",
    "\n",
    "# -------------------- Stratonovich trapezoid BSDE loss --------------------\n",
    "@eqx.filter_value_and_grad\n",
    "def bsde_loss(model, key, p, n_paths=4096, n_steps=64, T=1.0):\n",
    "    S, dW, dt = sim_paths(key, p, n_paths, n_steps, T)\n",
    "    # All below should be (paths, steps)\n",
    "    Y    = f_apply(model, p, S)\n",
    "    fp   = fprime_batch(model, p, S)\n",
    "    Z    = fp * sig_s(S, p)\n",
    "\n",
    "    # shape-safe \"next in time\"\n",
    "    S_n  = jnp.concatenate([S[..., 1:], S[..., -1:]], axis=-1)\n",
    "    Y_n  = jnp.concatenate([Y[..., 1:], Y[..., -1:]], axis=-1)\n",
    "    fp_n = fprime_batch(model, p, S_n)\n",
    "    Z_n  = fp_n * sig_s(S_n, p)\n",
    "\n",
    "    # Stratonovich trapezoid one-step: average drift/diffusion; stop-grad on next step\n",
    "    drift = 0.5*((p.delta*(Y + jax.lax.stop_gradient(Y_n))) - (S + jax.lax.stop_gradient(S_n))) * dt\n",
    "    diff  = 0.5*(Z + jax.lax.stop_gradient(Z_n)) * dW\n",
    "    resid = jax.lax.stop_gradient(Y_n) - (Y + drift + diff)\n",
    "\n",
    "    # Control variate on ΔW (OLS slope; stop-grad to avoid chasing noise)\n",
    "    beta  = (jnp.mean(resid*dW) / (jnp.mean(dW**2) + 1e-16))\n",
    "    resid = resid - jax.lax.stop_gradient(beta)*dW\n",
    "    return jnp.mean(resid**2)\n",
    "\n",
    "def make_opt(): return optax.chain(optax.clip(1.0), optax.adam(3e-3))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def train_step(model, opt, opt_state, key, p, n_paths, n_steps):\n",
    "    val, grads = bsde_loss(model, key, p, n_paths, n_steps)\n",
    "    updates, opt_state = opt.update(grads, opt_state, eqx.filter(model, eqx.is_inexact_array))\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return model, opt_state, val\n",
    "\n",
    "# -------------------- Independent acceptance checks --------------------\n",
    "\n",
    "def generator_residual(model, p, grid):\n",
    "    f   = lambda s: model(s, p.delta)\n",
    "    fp  = jax.vmap(jax.grad(f))(grid)\n",
    "    fpp = jax.vmap(lambda s: jax.grad(lambda u: jax.grad(f)(u))(s))(grid)\n",
    "    b   = jax.vmap(lambda s: b_ito(s, p))(grid)\n",
    "    v   = jax.vmap(lambda s: sig_s(s, p)**2)(grid)\n",
    "    return -grid + p.delta*f_apply(model, p, grid) - b*fp - 0.5*v*fpp\n",
    "\n",
    "def fd_bvp_solution(p, N=400):\n",
    "    s = jnp.linspace(0.0, 1.0, N+1).at[0].set(1e-12).at[-1].set(1.0-1e-12)\n",
    "    h = s[1]-s[0]\n",
    "    b = b_ito(s, p); v = sig_s(s, p)**2\n",
    "    a_lo = -0.5*v[1:-1]/h**2 + b[1:-1]/(2*h)\n",
    "    a_di =  p.delta*jnp.ones_like(s[1:-1]) + v[1:-1]/h**2\n",
    "    a_up = -0.5*v[1:-1]/h**2 - b[1:-1]/(2*h)\n",
    "    rhs  = s[1:-1]\n",
    "    rhs  = rhs.at[-1].add(-a_up[-1]*(1.0/p.delta))  # Y(1)=1/δ\n",
    "    from jax.lax.linalg import tridiagonal_solve\n",
    "    rhs2 = rhs[:, None]  # make rhs rank-2 for current JAX API\n",
    "    f_inner = tridiagonal_solve(a_lo, a_di, a_up, rhs2)[:, 0]\n",
    "    f = jnp.concatenate([jnp.array([0.0]), f_inner, jnp.array([1.0/p.delta])])\n",
    "    return s, f\n",
    "\n",
    "def f_closed_form_sym(s, delta):\n",
    "    # Eq. (22) holds in the symmetric case with δ=σ^2\n",
    "    V = (1.0/(2.0*delta*s))*(1.0 + ((1.0 - s)/s)*jnp.log1p(-s) - (s/(1.0 - s))*jnp.log(s))\n",
    "    return s*V\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b30cc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A_res_inf': 0.1954662771478498, 'B_bvp_err': 2.569036144584988, 'C_cf_err': 2.569056022316385, 'D_range_ok': True, 'sym_err': 1.1751915472856815}\n"
     ]
    }
   ],
   "source": [
    "# -------------------- Tiny driver (example: symmetric special case) --------------------\n",
    "p = Params(delta=0.04, mu1=0.02, mu2=0.02, sig1=0.2, sig2=0.2, rho=0.0)  # δ=σ^2 for Eq.(22)\n",
    "model = Head(jax.random.PRNGKey(0))\n",
    "opt   = make_opt(); opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "key = jax.random.PRNGKey(1)\n",
    "for _ in range(1500):\n",
    "    key, sub = jax.random.split(key)\n",
    "    model, opt_state, _ = train_step(model, opt, opt_state, sub, p, n_paths=4096, n_steps=64)\n",
    "\n",
    "# Acceptance checks\n",
    "grid = jnp.linspace(1e-6, 1.0-1e-6, 401)\n",
    "A_res_inf = jnp.max(jnp.abs(generator_residual(model, p, grid)))\n",
    "s_fd, f_fd = fd_bvp_solution(p)\n",
    "f_net = f_apply(model, p, s_fd)\n",
    "B_bvp_err = jnp.max(jnp.abs(f_net - f_fd))\n",
    "f_cf  = f_closed_form_sym(s_fd, p.delta)\n",
    "C_cf_err = jnp.max(jnp.abs(f_net - f_cf))\n",
    "D_range_ok = (jnp.min(f_net) >= -1e-6) & (jnp.max(f_net) <= 1.0/p.delta + 1e-6)\n",
    "sym_err = jnp.max(jnp.abs(f_apply(model, p, s_fd) + f_apply(model, p, 1.0 - s_fd) - 1.0/p.delta))\n",
    "print({\"A_res_inf\": float(A_res_inf),\n",
    "       \"B_bvp_err\": float(B_bvp_err),\n",
    "       \"C_cf_err\": float(C_cf_err),\n",
    "       \"D_range_ok\": bool(D_range_ok),\n",
    "       \"sym_err\": float(sym_err)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e496c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'S': (8, 5), 'dW': (8, 5), 'Y': (8, 5), 'fp': (8, 5), 'Z': (8, 5), 'S_n': (8, 5), 'Y_n': (8, 5), 'Z_n': (8, 5)}\n"
     ]
    }
   ],
   "source": [
    "# Quick shape diagnostics (small sizes)\n",
    "pp = Params(delta=0.04, mu1=0.02, mu2=0.02, sig1=0.2, sig2=0.2, rho=0.0)\n",
    "S, dW, dt = sim_paths(jax.random.PRNGKey(123), pp, n_paths=4, n_steps=5, T=1.0)\n",
    "Y    = f_apply(model, pp, S)\n",
    "fp   = fprime_batch(model, pp, S)\n",
    "Z    = fp * sig_s(S, pp)\n",
    "S_n  = jnp.concatenate([S[..., 1:], S[..., -1:]], axis=-1)\n",
    "Y_n  = jnp.concatenate([Y[..., 1:], Y[..., -1:]], axis=-1)\n",
    "fp_n = fprime_batch(model, pp, S_n)\n",
    "Z_n  = fp_n * sig_s(S_n, pp)\n",
    "print({\n",
    "    'S': S.shape,\n",
    "    'dW': dW.shape,\n",
    "    'Y': Y.shape,\n",
    "    'fp': fp.shape,\n",
    "    'Z': Z.shape,\n",
    "    'S_n': S_n.shape,\n",
    "    'Y_n': Y_n.shape,\n",
    "    'Z_n': Z_n.shape,\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Model (JAX CUDA)",
   "language": "python",
   "name": "model-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
