{
 "cells": [
  {"cell_type":"markdown","metadata":{},"source":["# Train MacroFinanceNet â€” Demo (FAST)\n","\n","This demo runs a small training loop (few epochs, small batch) to show the loss decreasing and prints symmetric-state q."]},
  {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os, time, json\n","import numpy as np\n","import jax, jax.numpy as jnp\n","import equinox as eqx\n","import optax\n","import matplotlib.pyplot as plt\n","from bsde_dsgE.models.macro_solver import Config as NetCfg, MacroFinanceNet, evaluate_symmetric\n","from bsde_dsgE.models.probab01_equations import Config as EqCfg, compute_dynamics, q_symmetric_analytic\n","FAST = True\n","J=5; steps = 8 if FAST else 64; paths = 512 if FAST else 4096; dt = 0.001\n","net_cfg = NetCfg(J=J); eq_cfg = EqCfg(J=J)\n","key = jax.random.PRNGKey(0)\n","model = MacroFinanceNet(net_cfg, key)\n","opt = optax.adam(1e-4)\n","opt_state = opt.init(eqx.filter(model, eqx.is_inexact_array))\n","def batch_loss(model, key):\n","    B = paths\n","    key_eta, key_dir, key_dW = jax.random.split(key, 3)\n","    eta = jax.random.uniform(key_eta, (B, eq_cfg.N_ETA), minval=0.2, maxval=0.8)\n","    raw = jax.random.uniform(key_dir, (B, eq_cfg.J)); z_full = raw / jnp.sum(raw, axis=1, keepdims=True)\n","    z = z_full[:, : eq_cfg.N_ZETA]; Om0 = jnp.hstack([eta, z])\n","    dWs = jax.random.normal(key_dW, (steps, B, eq_cfg.J)) * jnp.sqrt(dt)\n","    def scan_fn(carry, dW_i):\n","        Om = carry\n","        q, s, r = model(Om)\n","        drift, vol, h, Z = compute_dynamics(eq_cfg, Om, q, s, r)\n","        Om1 = Om + jnp.einsum('bij,bi->bj', vol, dW_i) + drift * dt\n","        q1 = q - h * dt + jnp.einsum('bij,bi->bj', Z, dW_i)\n","        qh, _, _ = model(Om1)\n","        l = jnp.mean(jnp.sum((qh - q1)**2, axis=1))\n","        return Om1, l\n","    _, losses = jax.lax.scan(scan_fn, Om0, dWs)\n","    return jnp.mean(losses)\n","@eqx.filter_jit\n","def train_step(model, opt_state, key):\n","    loss, grads = eqx.filter_value_and_grad(batch_loss)(model, key)\n","    updates, opt_state = opt.update(grads, opt_state, model)\n","    model = eqx.apply_updates(model, updates)\n","    return model, opt_state, loss\n","loss_hist = []\n","print({'analytic_q': q_symmetric_analytic(a=0.1, psi=5.0, rho=0.03)})\n","for ep in range(1, (200 if FAST else 1000)+1):\n","    key, k = jax.random.split(key)\n","    model, opt_state, loss = train_step(model, opt_state, k)\n","    loss_hist.append(float(loss))\n","    if ep % (50 if FAST else 100) == 0: print({'ep': ep, 'loss': float(loss)})\n","etas = (0.3,0.4,0.5,0.6,0.7)\n","q, s, r = evaluate_symmetric(net_cfg, model, etas)\n","print('q@symmetric (demo):', np.array(q))\n","# Plot loss history\n","fig, ax = plt.subplots(figsize=(5,3))\n","ax.plot(range(1, len(loss_hist)+1), loss_hist)\n","ax.set_xlabel('epoch'); ax.set_ylabel('loss'); ax.set_title('Training loss (demo)')\n","fig.tight_layout(); fig\n"]}
 ],
 "metadata": {"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11"}},
 "nbformat":4, "nbformat_minor":5}
