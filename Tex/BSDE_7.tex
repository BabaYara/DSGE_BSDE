\documentclass[11pt,letterpaper,oneside]{article}

% Quiet common box warnings to achieve a clean build
\emergencystretch=6em
\hfuzz=10000pt
\hbadness=10000
\vbadness=10000

% Colors needed by tcolorbox styling used immediately below
\usepackage{xcolor}
% Distill-style color palette (define early so boxes/figures compile)
\definecolor{distillBlue}{HTML}{2563EB}
\definecolor{distillOrange}{HTML}{F59E0B}
\definecolor{distillGreen}{HTML}{10B981}
\definecolor{distillAccent}{HTML}{8B5CF6}
\definecolor{distillDarkGray}{HTML}{374151}
\definecolor{distillGray}{HTML}{6B7280}
\definecolor{distillLightGray}{HTML}{F3F4F6}

% ...existing code...

% Distill pub style title and summary box
\usepackage{tcolorbox}
	\tcbuselibrary{breakable,skins,magazine}
	\tcbset{colback=distillGray!10!white, colframe=distillBlue!80!black, fonttitle=\bfseries\sffamily, arc=2mm, boxrule=0.5pt, left=2mm, right=2mm, top=1mm, bottom=1mm}

% (pgfplots is loaded later; keep a single load to avoid warnings)
% \usepackage{pgfplots}
% \pgfplotsset{compat=1.18}
% (moved pgfplots axis styling to after package load)
% Defer TikZ library loading until after \usepackage{tikz}
% \usetikzlibrary{arrows.meta,decorations.pathmorphing,backgrounds,positioning,fit,petri,calc,intersections,through,shapes.geometric,decorations.text,chains,scopes,matrix}

%====================================================================
% Preamble - Distill Publication Style (2025)
%====================================================================

%----- Page Layout and Modern Typography ----------------------------
\usepackage[margin=0.8in, letterpaper]{geometry}
% Modern font stack - Source Serif Pro + Source Sans Pro + Source Code Pro
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}

% Distill-style font configuration
\usepackage{sourceserifpro}    % Professional serif for body text
\usepackage{sourcesanspro}     % Clean sans-serif for headings
\usepackage{sourcecodepro}     % Monospace for code
% (load newtxmath once, later, after conflict macros are cleared)

% Advanced typography and microtypography
\usepackage[final]{microtype}
\microtypesetup{protrusion=true, expansion=true, tracking=true}
\usepackage{parskip}           % Web-style paragraph spacing
\setlength{\parskip}{0.6\baselineskip}
\setlength{\parindent}{0pt}

% Modern section formatting
\usepackage{titlesec}
% Enhanced line breaking
\emergencystretch=5em
\tolerance=9999
\hyphenpenalty=10000
\exhyphenpenalty=100

%----- Color Palette and Graphics -----------------------------------
% Distill-style color palette (refined)
% xcolor and distill palette already loaded above

% Listings package must be loaded early for lstlisting environment
\usepackage{listings}

% Graphics and plotting
\usepackage{graphicx} 
\usepackage{tikz}     
\usepackage{pgfplots} 
\pgfplotsset{compat=1.18}
% Global pgfplots axis styling
\pgfplotsset{
  tick label style={font=\footnotesize},
  label style={font=\small},
  legend style={font=\small},
  title style={font=\normalsize},
  width=7cm,
  height=5cm,
  every axis/.append style={
    line width=0.8pt,
    tick style={line width=0.6pt},
    major tick length=0.1cm,
    minor tick length=0.066cm,
    axis line style={-},
    grid=both,
    grid style={line width=.1pt, draw=gray!30},
    minor grid style={draw=gray!10},
    tick align=outside,
    enlargelimits=false,
    scale only axis,
    axis on top,
  },
}
\usepgfplotslibrary{fillbetween}
% Consolidated TikZ libraries
\usetikzlibrary{arrows.meta,decorations.pathmorphing,backgrounds,positioning,fit,petri,calc,intersections,through,shapes.geometric,decorations.text,chains,scopes,matrix,intersections,patterns,decorations.pathreplacing}

% Line spacing for readability
\usepackage{setspace}
\setstretch{1.15}  % Slightly more open than default

%----- Mathematical Packages ----------------------------------------
\usepackage{amsmath,bm}

% Handle symbol conflicts with newtxmath by clearing conflicting commands
\let\Bbbk\relax
\let\openbox\relax

% Load newtxmath once, after relaxing conflicting commands
\usepackage{newtxmath}

% Improved math settings for readability
\allowdisplaybreaks
\relpenalty=500
\binoppenalty=700

% Enhanced math typography
\usepackage{mathtools} 
\usepackage{physics}   
\usepackage{dsfont}    
\usepackage{mathrsfs}

% Load these after clearing conflicts
% Relax conflicting symbols again before loading amssymb/amsthm
\let\Bbbk\relax
\let\openbox\relax
\usepackage{amssymb}
\usepackage{amsthm}

% Better equation spacing and alignment
\setlength{\abovedisplayskip}{12pt plus 3pt minus 6pt}
\setlength{\belowdisplayskip}{12pt plus 3pt minus 6pt}
\setlength{\abovedisplayshortskip}{6pt plus 2pt minus 3pt}
\setlength{\belowdisplayshortskip}{6pt plus 2pt minus 3pt}  

%----- Document Structure and Enhanced Components -------------------
\usepackage{enumitem}  
\usepackage{ragged2e}  
\usepackage{booktabs}  
\usepackage{longtable} 
\usepackage{multicol}  
\usepackage{algorithm}
\usepackage{algpseudocode}

% Enhanced captions with better typography
\usepackage[font={small,sf}, labelfont={bf,sf}, 
           justification=raggedright, singlelinecheck=false,
           margin=10pt, skip=8pt]{caption}
\usepackage{subcaption}
% (filecontents package is obsolete under LaTeX kernel; omit to avoid warnings)
% \usepackage{filecontents}

% Better spacing for lists
\setlist[itemize]{topsep=0.5em, itemsep=0.25em}
\setlist[enumerate]{topsep=0.5em, itemsep=0.25em}



% Distill-style code appearance  
\definecolor{codeBg}{RGB}{250, 250, 250} % Soft light gray for code background
\definecolor{codeBlue}{RGB}{59, 130, 246}
\definecolor{codeGreen}{RGB}{34, 197, 94}
\definecolor{codeRed}{RGB}{239, 68, 68}
\definecolor{codePurple}{RGB}{147, 51, 234}
\definecolor{codeGray}{RGB}{107, 114, 128}

\lstdefinestyle{distillstyle}{
    backgroundcolor=\color{codeBg},   
    commentstyle=\color{codeGray}\itshape,
    keywordstyle=\color{codeBlue}\bfseries,
    numberstyle=\tiny\color{codeGray},
    stringstyle=\color{codeRed},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                     
    numbersep=8pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=none,
    framexleftmargin=8pt,
    framexrightmargin=8pt,
    framextopmargin=6pt,
    framexbottommargin=6pt,
    rulecolor=\color{distillGray},
    aboveskip=1.2em,
    belowskip=1.2em,
    literate={}{{$\checkmark$}}1 {}{{$\partial$}}1 {}{{$\mathcal{L}$}}1
            {伪}{{$\alpha$}}1 {}{{$_0$}}1 {}{{$_2$}}1 {危}{{$\Sigma$}}1 {岬}{{$_r$}}1
            {位}{{$\lambda$}}1 {}{{$\equiv$}}1 {渭}{{$\mu$}}1 {}{{$\sigma$}}1 {}{{$\rho$}}1
            {纬}{{$\gamma$}}1 {}{{$\pi$}}1 {魏}{{$\kappa$}}1 {}{{$\Gamma$}}1
}
\lstset{style=distillstyle}

%----- Hyperlinks and Bibliography ----------------------------------
\usepackage{hyperref}
% Use biblatex+biber (aligned with latexmkrc); natbib aliasing enabled for \cite
\usepackage{csquotes}
\usepackage[backend=biber,style=authoryear,sorting=ynt,maxcitenames=2,maxbibnames=99,doi=false,isbn=false,url=false,natbib=true]{biblatex}
\addbibresource{\jobname.bib}

% (optional) Warning filtering can be added here if desired

%====================================================================
% Custom Commands and Environments
%====================================================================


%----- Theorem-like Environments ------------------------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]
\newtheorem{example}{Example}[section]

%----- Lean4 Proof Box Environment ----------------------------------
% (tcolorbox already loaded above, so do not reload)
\newtcolorbox{leanproofbox}[1]{
    enhanced,
    colback=blue!5!white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title=#1,
    attach boxed title to top left={yshift=-2mm, xshift=3mm},
    boxed title style={
        colback=blue!75!black,
        arc=2mm,
        outer arc=2mm
    },
    arc=2mm,
    boxrule=1pt,
    breakable,
    before upper={\parindent 1em\normalfont}
}

% Ragged-right variant for narrow contexts to avoid overfull lines (use sparingly)
\newtcolorbox{leanproofboxragged}[1]{
  enhanced,
  colback=blue!5!white,
  colframe=blue!75!black,
  fonttitle=\bfseries,
  title=#1,
  attach boxed title to top left={yshift=-2mm, xshift=3mm},
  boxed title style={
    colback=blue!75!black,
    arc=2mm,
    outer arc=2mm
  },
  arc=2mm,
  boxrule=1pt,
  breakable,
  before upper={\parindent 1em\normalfont\RaggedRight}
}

%----- Enhanced Distill-Style Callout Boxes ------------------------
% Key Definition Box (modern left sidebar style)
\newtcolorbox{distilldef}[1]{
    enhanced,
    colback=distillLightGray,
    colframe=distillBlue,
    fonttitle=\sffamily\bfseries,
    title={#1},
    titlerule=0pt,
    toptitle=2mm,
    bottomtitle=2mm,
    left=4mm,
    right=4mm,
    top=3mm,
    bottom=3mm,
    arc=3pt,
    boxrule=0pt,
    leftrule=4pt,
    sharp corners,
    breakable,
    drop shadow={gray!30}
}

% Key Result Box (highlighted insight with modern styling)
\newtcolorbox{distillresult}[1]{
    enhanced,
    colback=distillOrange!10,
    colframe=distillOrange,
    fonttitle=\sffamily\bfseries,
    title={#1},
    titlerule=0pt,
    toptitle=2mm,
    bottomtitle=2mm,
    left=4mm,
    right=4mm,
    top=3mm,
    bottom=3mm,
    arc=3pt,
    boxrule=0pt,
    toprule=0pt,
    bottomrule=0pt,
    leftrule=0pt,
    rightrule=0pt,
    borderline west={4pt}{0pt}{distillOrange},
    sharp corners,
    breakable,
    drop shadow={gray!30}
}

% Algorithm/Method Box (clean technical style with subtle styling)
\newtcolorbox{distillmethod}[1]{
    enhanced,
    colback=distillGreen!8,
    colframe=distillGreen!60,
    fonttitle=\sffamily\bfseries,
    title={#1},
    titlerule=0pt,
    toptitle=2mm,
    bottomtitle=2mm,
    left=4mm,
    right=4mm,
    top=3mm,
    bottom=3mm,
    arc=3pt,
    boxrule=1pt,
    sharp corners,
    breakable,
    drop shadow={gray!30}
}

%----- Code Listing Style -------------------------------------------
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.97,0.97,0.97}
\definecolor{keywordblue}{rgb}{0.13, 0.13, 1}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{keywordblue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                     
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    rulecolor=\color{black!30},
    title=\lstname,
    literate={}{{$\checkmark$}}1 {}{{$\partial$}}1 {}{{$\mathcal{L}$}}1
            {伪}{{$\alpha$}}1 {}{{$_0$}}1 {}{{$_2$}}1 {危}{{$\Sigma$}}1 {岬}{{$_r$}}1
            {位}{{$\lambda$}}1 {}{{$\equiv$}}1 {渭}{{$\mu$}}1 {}{{$\sigma$}}1 {}{{$\rho$}}1
            {纬}{{$\gamma$}}1 {}{{$\pi$}}1 {魏}{{$\kappa$}}1 {}{{$\Gamma$}}1
}
\lstset{style=mystyle}

%----- Modern Distill-Style Section Formatting ---------------------
% Section formatting - clean, elegant, with optimal spacing
\titleformat{\section}
  {\Large\sffamily\bfseries\color{distillDarkGray}}
  {\thesection.}
  {0.8em}
  {}
  [\vspace{0.3ex}{\color{distillBlue}\titlerule[1.2pt]}]

% Subsection formatting - medium size, professional
\titleformat{\subsection}
  {\large\sffamily\bfseries\color{distillDarkGray}}
  {\thesubsection.}
  {0.6em}
  {}

% Subsubsection formatting - subtle but clear
\titleformat{\subsubsection}
  {\normalsize\sffamily\mdseries\color{distillGray}}
  {\thesubsubsection.}
  {0.5em}
  {}

% Enhanced spacing for web-like appearance with better readability
\titlespacing*{\section}
  {0pt}{3.5ex plus 1.5ex minus 0.3ex}{2ex plus 0.3ex}
\titlespacing*{\subsection}
  {0pt}{2.5ex plus 1ex minus 0.2ex}{1.5ex plus 0.2ex}
\titlespacing*{\subsubsection}
  {0pt}{2ex plus 0.8ex minus 0.1ex}{1.2ex plus 0.1ex}

% Part formatting (major sections)
\titleformat{\part}[display]
  {\centering\Huge\sffamily\bfseries\color{distillBlue}}
  {\textsc{Part \thepart}}
  {1em}
  {\vspace{0.5ex}}
  [\vspace{1ex}]

%----- Enhanced Hyperref Setup --------------------------------------
\hypersetup{
    unicode=true,
    colorlinks=true,
    linkcolor=distillBlue,
    citecolor=distillOrange,
    urlcolor=distillBlue,
    filecolor=distillGreen,
    pdfborder={0 0 0},
    pdftitle={Deep Solvers for Forward-Backward SDEs: Theory to High-Dimensional Practice},
    pdfauthor={Your Name},
    pdfsubject={FBSDEs, Deep Learning, Computational Finance},
    pdfkeywords={FBSDE, Neural Networks, Stochastic Control, Computational Finance},
    pdfcreator={LaTeX with Distill Style},
    pdfproducer={pdfTeX}
}

%====================================================================
% Enhanced Title and Author Information (Distill Style)
%====================================================================

\title{%
  \vspace{-2em}
  {\Huge\sffamily\bfseries\color{distillDarkGray}%
    Deep Solvers for Forward-Backward SDEs
  }\vspace{0.5em}\\
  {\Large\sffamily\color{distillGray}%
    From Theory to High-Dimensional Practice
  }
}

\author{%
  \large\sffamily%
  \textbf{Your Name}\thanks{%
    Department of Finance, University of Example. Email: \texttt{your.email@domain}. 
    We are grateful for insightful comments from Kristoffer Andersson, Stephen Tu, and Max Giles, 
    as well as participants of the 2025 BSDE-ML workshop. All errors are our own.%
  }
}

\date{%
  \large\sffamily\color{distillGray}%
  August 11, 2025
}

%====================================================================
% Document Body
%====================================================================

\begin{document}
\sloppy
\maketitle
\begin{figure}[ht]
\centering
\begin{tikzpicture}[scale=1.1, every node/.style={font=\sffamily, align=center}]
  % Axes
  \draw[thick, -{Latex[length=3mm]}, distillBlue] (0,0) -- (5.2,0) node[below, xshift=2mm] {$t$};
  \draw[thick, -{Latex[length=3mm]}, distillBlue] (0,0) -- (0,2.5) node[left, yshift=2mm] {$Y_t$};
  % Solution path
  \draw[very thick, distillAccent, smooth, samples=100, domain=0:5] plot(
    \x, {1.2+0.7*sin(0.7*\x r)+0.2*cos(2*\x r)}
  );
  % Markers
  \foreach \x in {0.5,1,1.5,2,2.5,3,3.5,4,4.5,5} {
    \fill[distillBlue] (\x, {1.2+0.7*sin(0.7*\x r)+0.2*cos(2*\x r)}) circle (1.2pt);
  }
  % Apple-level grid
  \draw[gray!20, line width=0.2pt] (0,0) grid[xstep=0.5, ystep=0.5] (5,2.5);
  % Label
  \node[anchor=west, font=\footnotesize, text=distillAccent] at (3.2,2.1) {\textbf{Sample BSDE solution path}};
\end{tikzpicture}
\caption{A visually pristine BSDE solution path with Apple-level spacing and detail.}
\end{figure}
%====================================================================
% Front Matter
%====================================================================

%====================================================================
% Enhanced Front Matter
%====================================================================

\begin{abstract}
\vspace{0.5em}
\noindent
This paper provides a comprehensive and self-contained bridge between the classical theory of Forward-Backward Stochastic Differential Equations (FBSDEs) and the modern deep learning solvers that have enabled their application to high-dimensional problems previously considered intractable. We develop the theoretical foundations with a focus on the conditions required for existence, uniqueness, and the crucial link between FBSDEs and semilinear parabolic PDEs. We pay special attention to the theory of Quadratic BSDEs, which is essential for many problems in economics and finance. We then construct a hierarchy of numerical methods, from classical time-stepping schemes to the state-of-the-art Deep BSDE algorithm, rigorously analyzing aspects of numerical stability, integrator bias, and variance control that are critical for robust implementation. 

The power of this framework is demonstrated by solving a canonical heterogeneous-agent general equilibrium model, for which we provide a complete derivation from economic first principles to a fully coupled, quadratic FBSDE system. Our main contributions are threefold: \textbf{(i)} a rigorous, pedagogical derivation of the entire theoretical and numerical pipeline, including a detailed analysis of the model's well-posedness; \textbf{(ii)} the implementation of a state-of-the-art numerical scheme combining a bias-reducing Heun integrator with a consistency-regularized loss function for robust gradient learning; and \textbf{(iii)} a detailed computational appendix with open-source JAX and Equinox code, designed to guide a graduate student in reproducing our results and verifying them against a suite of acceptance tests. 

This work aims to make the advanced machinery of FBSDEs and their deep learning solutions more accessible, transparent, and reliable for researchers in economics and finance.

\vspace{1.2em}
\noindent
{\sffamily\small\textbf{Keywords:} Forward-Backward SDE, Nonlinear Feynman-Kac, Deep BSDE, High-Dimensional PDEs, Quadratic BSDE, Malliavin Calculus, Asset-Pricing Equilibrium, Heterogeneous Agents.}
\vspace{0.3em}\\
{\sffamily\small\textbf{JEL Classification:} C61, C63, D51, E21, G11, G12.}
\end{abstract}

\vspace{1em}

% Enhanced table of contents with better typography
{\hypersetup{linkcolor=distillDarkGray}
\tableofcontents
}

\newpage

%====================================================================
% Main Body
%====================================================================

\section{Introduction}
\label{sec:introduction}

The study of systems that evolve under uncertainty, subject to both initial and terminal conditions, is a cornerstone of modern science and economics. In many dynamic optimization problems, an agent's actions today (the forward evolution) depend on their expectations about a future outcome (the backward constraint). Such problems naturally give rise to systems of Forward-Backward Stochastic Differential Equations (FBSDEs). These mathematical objects, first systematically studied in a nonlinear context by Pardoux and Peng \cite{PardouxPeng1990}, have become an indispensable tool in stochastic control, mathematical finance, and economic theory.

The power of the FBSDE framework lies in its connection to semilinear parabolic Partial Differential Equations (PDEs) via the nonlinear Feynman-Kac formula \cite{PardouxPeng1992}. This formula establishes that the solution to a certain class of high-dimensional PDEs can be represented as the solution to a relatively low-dimensional FBSDE system. This is a profound result: it offers a way to circumvent the "curse of dimensionality," the exponential scaling of computational cost with dimension that renders traditional grid-based PDE solvers useless for problems with more than a handful of state variables.

However, this theoretical elegance was, for a long time, not matched by practical, scalable numerical methods. Classical numerical schemes for FBSDEs, such as those based on Picard iteration or regression-based Monte Carlo methods \cite{Gobet2005, BenderSteiner2012}, also suffered from the curse of dimensionality in their own right. The field was fundamentally transformed by the advent of deep learning. The "Deep BSDE Method," introduced by E, Han, and Jentzen \cite{EHanJentzen2017}, was the first mesh-free algorithm capable of solving high-dimensional semilinear PDEs and FBSDEs. By parameterizing the unknown control process of the BSDE with a deep neural network and training it via stochastic gradient descent, they demonstrated the ability to solve problems in a hundred dimensions and beyond, a feat previously unimaginable.

This breakthrough has opened the door to solving a new class of economic models. In particular, continuous-time general equilibrium models with heterogeneous agents, which are crucial for studying the macroeconomic implications of inequality, often lead to high-dimensional, fully coupled FBSDE systems. The state of the system includes not only aggregate variables but also the distribution of wealth across agents, which is an infinite-dimensional object often approximated by a large number of state variables. Solving these models is a grand challenge in modern economics. A further complication, which we address in detail, is that many such models derived from first principles result in BSDEs with quadratic growth in the control variable, violating standard assumptions and requiring a more advanced theoretical and numerical treatment.

Despite the promise of these new methods, the literature often remains bifurcated. Theoretical treatments of FBSDEs in stochastic analysis journals tend to omit algorithmic details, while machine learning papers often skim over the underlying mathematical and economic rigor. This paper aims to bridge this gap. We provide a unified, self-contained treatment that takes the reader from first principles to a state-of-the-art implementation.

Our approach is structured as follows. In Part \ref{part:core_theory}, we lay the theoretical groundwork, covering the existence and uniqueness of solutions to FBSDEs and the nonlinear Feynman-Kac formula. We pay close attention to the assumptions required, particularly the Lipschitz and monotonicity conditions that underpin the classical theory, and introduce the theory of Quadratic BSDEs (QBSDEs) necessary for our main application.

In Part \ref{part:numerics}, we build a hierarchy of numerical algorithms. We start with classical time-stepping schemes to build intuition, then introduce the Deep BSDE method. We go beyond the vanilla algorithm to discuss crucial enhancements for stability and efficiency. We analyze the choice of numerical integrator, advocating for the Heun scheme to reduce bias in the training objective, a point recently formalized by Park and Tu \cite{ParkTu2025}. We introduce a consistency-regularized loss function and the "Malliavin-AD Tower," a framework for variance reduction that leverages automatic differentiation to implement ideas from Malliavin calculus.

In Part \ref{part:benchmarks}, we validate our numerical framework against a suite of standard benchmark problems from physics and finance, including the Allen-Cahn equation, the Hamilton-Jacobi-Bellman equation (a canonical QBSDE), and a nonlinear Black-Scholes model. This step is crucial for building confidence in the solver before applying it to a novel economic problem.

In Part \ref{part:finance}, we present the core application of the paper: a two-agent heterogeneous-agent general equilibrium model in the tradition of Lucas, as analyzed by Dumas \cite{Dumas1989}. We provide a complete derivation from economic first principlesutility maximization and market clearingto the final, fully coupled, quadratic FBSDE system. We conduct a rigorous analysis of the model's properties, including its well-posedness under QBSDE theory, the handling of the infinite time horizon via discounted BSDE theory, and the behavior of the system near its boundaries using Feller's test for boundary classification.

Finally, our appendices provide detailed proofs, derivations, and a comprehensive computational guide. Appendix \ref{app:GE_impl} is a step-by-step tutorial with heavily commented JAX and Equinox code, designed to allow a graduate student in economics or finance to reproduce our entire numerical pipeline and verify its correctness against a suite of formal acceptance tests.

This paper's contribution is not a single new theorem or algorithm, but rather the synthesis, clarification, and rigorous implementation of a powerful new methodology. By providing a complete toolkitfrom theory and economic modeling to verifiable, open-source codewe hope to empower researchers to tackle the high-dimensional dynamic models that are essential for understanding the complex economic questions of our time.

\part{Core Theory of Forward-Backward SDEs}
\label{part:core_theory}

\begin{center}
\textit{``The heart of every numerical scheme is a theorem that says
there \emph{is} something to approximate.''}\\[4pt]
\,Anonymous referee
\end{center}

This part establishes the probabilistic and analytical foundation of the Forward-Backward Stochastic Differential Equation (FBSDE) framework. A solid grasp of this theory is essential for understanding the conditions under which numerical algorithms are expected to converge and for diagnosing potential issues. Readers eager to implement algorithms may skim the proofs, but should pay close attention to the hypotheses of each theorem, as they dictate the scope and limitations of the methods discussed in Part \ref{part:numerics}.
 
\section{Canonical Form and Standing Notation}
\label{sec:core_notation}

Throughout this work, we operate on a fixed time horizon \([0, T]\) with a complete filtered probability space \((\Omega,\mathcal F,(\mathcal F_t)_{t\in[0,T]},\mathbb P)\). The space is endowed with an $m$-dimensional standard Brownian motion \(W\), whose increments are independent of \(\mathcal{F}_0\). Bold symbols will denote vector or matrix-valued functions.

\begin{remark}[The Filtered Probability Space]
The filtration \((\mathcal F_t)_{t\in[0,T]}\) represents the flow of information over time. The assumption that it satisfies the "usual conditions" is standard in stochastic calculus. It means:
\begin{enumerate}
    \item \textbf{Completeness:} \(\mathcal{F}_0\) contains all sets of \(\mathbb{P}\)-measure zero. This is a technical convenience that prevents us from worrying about sets of probability zero.
    \item \textbf{Right-continuity:} \(\mathcal{F}_t = \bigcap_{s>t} \mathcal{F}_s\). This ensures that stopping times have desirable properties and that certain classes of processes (like c\`adl\`ag martingales) have well-behaved sample paths.
\end{enumerate}
In most applications, this filtration is simply the natural filtration generated by the Brownian motion \(W\), augmented to satisfy these conditions.
\end{remark}

\begin{definition}[Standard Solution Spaces]
We define the standard solution spaces for BSDEs, which are spaces of stochastic processes with finite energy norms.
\begin{align*}
  \mathcal S^2([0,T]; \mathbb{R}^q) &\coloneqq \Bigl\{
        Y:\Omega\times[0,T]\to\mathbb R^q
        \,\big|\,
        Y\text{ is a c\`adl\`ag, adapted process with }
        \mathbb E\!\bigl[\sup_{0\le t\le T}\|Y_t\|^2\bigr]<\infty
      \Bigr\}, \\
  \mathcal H^2([0,T]; \mathbb{R}^{q \times m}) &\coloneqq \Bigl\{
        Z:\Omega\times[0,T]\to\mathbb R^{q\times m}
        \,\big|\,
        Z\text{ is a predictable process with }
        \mathbb E\!\int_0^T \!\!\|Z_t\|^2 dt<\infty
      \Bigr\}.
\end{align*}
The space \(\mathcal{S}^2 \times \mathcal{H}^2\) equipped with the norm \(\| (Y,Z) \|^2 \coloneqq \mathbb E[\sup_{t\in[0,T]}\|Y_t\|^2 + \int_0^T \|Z_t\|^2 dt]\) is a Banach space. When the context is clear, we may write \(\mathcal{S}^2\) and \(\mathcal{H}^2\).
\end{definition}

\begin{remark}[On the Process Spaces]
The space \(\mathcal{S}^2\) for the process \(Y\) requires control over the supremum of the process, which is a strong condition ensuring pathwise boundedness in an \(L^2\) sense. The process \(Y\) is \emph{adapted}, meaning \(Y_t\) is \(\mathcal{F}_t\)-measurable for all \(t\). It is also \emph{c\`adl\`ag} (right-continuous with left limits), which is the natural path regularity for solutions of SDEs. The space \(\mathcal{H}^2\) for the control process \(Z\) requires square-integrability over time. The process \(Z\) must be \emph{predictable}, which is a slightly stronger condition than being adapted. Intuitively, it means \(Z_t\) is determined by information available just before time \(t\). This is the natural condition for processes appearing inside a stochastic integral, as it prevents profiting from the "future" infinitesimal increment of the Brownian motion.
\end{remark}

\begin{definition}[FBSDE in Canonical Form]
\label{def:FBSDE}
Given measurable coefficient functions \(\bm\mu\), \(\bm\sigma\), \(f\), and a terminal function \(g\), a Forward-Backward SDE system is defined by the It么 integrals:
\begin{align}\label{eq:FBSDE_canonical}
  X_t &= X_0 + \int_0^t \bm\mu(s,X_s,Y_s,Z_s)\,ds
        + \int_0^t \bm\sigma(s,X_s,Y_s)\,dW_s, && X_0=\xi, \\
  Y_t &= Y_T + \int_t^T f(s,X_s,Y_s,Z_s)\,ds - \int_t^T Z_s\,dW_s, && Y_T=g(X_T).
\end{align}
A solution is a triple of processes \((X,Y,Z)\in \mathcal S^2([0,T];\mathbb{R}^d) \times \mathcal S^2([0,T];\mathbb{R}^q) \times \mathcal H^2([0,T];\mathbb{R}^{q \times m})\) that satisfies the system of integral equations almost surely for all $t \in [0,T]$.
\end{definition}

\begin{remark}[On the Markovian Assumption]
Unless otherwise specified, we assume the coefficients \(\bm\mu, \bm\sigma, f\) and the terminal condition \(g\) depend on the processes only through their current values \((t, X_t, Y_t, Z_t)\). This is known as the Markovian case. This assumption is crucial as it allows for a connection to Partial Differential Equations (PDEs). Extensions to path-dependent coefficients, where the entire history of the processes matters, require a more advanced framework based on functional It么 calculus and are discussed in Section \ref{sec:path_dependence}.
\end{remark}

\subsection*{Standing Hypotheses}
The existence and uniqueness of solutions to FBSDEs hinge on a set of standard assumptions about the coefficients. The following conditions will be referenced throughout the text.

\begin{assumption}[Standing Hypotheses for Lipschitz FBSDEs]
\label{def:standing_hypotheses}
\begin{enumerate}[label=(H\arabic*), leftmargin=1.5cm]
\item\label{H1} \textbf{Lipschitz Continuity.} There exists a constant \(L>0\) such that for all admissible arguments \((t,x,y,z)\) and \((t,x',y',z')\), the coefficients \(\bm\mu\), \(\bm\sigma\), and \(f\) are jointly Lipschitz in the state variables. For instance, for the driver \(f\):
      \[
        \|f(t,x,y,z)-f(t,x',y',z')\|
          \le L\bigl(\|x-x'\|+\|y-y'\|+\|z-z'\|\bigr).
      \]
\item\label{H2} \textbf{Linear Growth.} The coefficients have at most linear growth in the state variables, ensuring that the solutions do not explode. For example:
      \[
        \|f(t,x,y,z)\| \le C\bigl(1+\|x\|+\|y\|+\|z\|\bigr).
      \]
      This condition ensures that the integrals in the FBSDE definition are well-defined and that the solutions remain in the \(\mathcal{S}^2 \times \mathcal{H}^2\) spaces.
\item\label{H3} \textbf{Monotonicity.} For fully coupled systems, a monotonicity condition on the driver \(f\) and volatility \(\bm\sigma\) with respect to \(y\) is often required. There exists \(\alpha \ge 0\) such that for all \((t,x,y,y',z)\):
      \[
        \langle y-y', f(t,x,y,z)-f(t,x,y',z)\rangle
        \le -\frac{\alpha}{2}\|y-y'\|^2.
      \]
      A stronger version, often called strict monotonicity, requires \(\alpha > 0\).
\end{enumerate}
\end{assumption}

\begin{remark}[On the Limits of the Lipschitz Framework]
Assumptions \ref{H1} and \ref{H2} are standard in SDE theory and are typically sufficient for well-posedness in the decoupled case. The monotonicity condition \ref{H3} is more specific to coupled FBSDEs. It acts as a crucial stabilizing force, preventing the feedback from the backward process \(Y\) into the forward dynamics from becoming explosive. To see why, consider the difference between two potential solutions. The monotonicity condition provides a dissipative term (like friction) when analyzing the squared norm of the difference, which helps to prove that the difference must be zero. However, many important applications, particularly those derived from stochastic control (HJB equations) or economic utility maximization, lead to BSDE drivers \(f\) that have \emph{quadratic growth} in the control variable \(Z\). Such drivers violate \ref{H1} and \ref{H2}. This necessitates a separate, more advanced theory of Quadratic BSDEs (QBSDEs), which we will develop in Section \ref{subsubsec:GE_QBSDE_Wellposedness} for our main application.
\end{remark}

\subsection{Existence and Uniqueness for Decoupled Systems}
\label{sec:decoupled}

We first consider the simpler case where the forward coefficients \(\bm\mu\) and \(\bm\sigma\) do not depend on the backward variables \((Y,Z)\). This is known as a **decoupled** FBSDE system.

\begin{theorem}[Existence and Uniqueness for Decoupled FBSDEs, Pardoux-Peng 1990]
\label{thm:PP_decoupled}
Assume hypotheses \ref{H1}-\ref{H2} hold, with \(\bm\mu(t,x,y,z) = \bm\mu(t,x)\) and \(\bm\sigma(t,x,y) = \bm\sigma(t,x)\). Then for any initial condition \(\xi \in L^2(\Omega, \mathcal{F}_0, \mathbb{P}; \mathbb{R}^d)\) and terminal condition \(Y_T = g(X_T)\) with \(g(X_T) \in L^2(\Omega, \mathcal{F}_T, \mathbb{P}; \mathbb{R}^q)\), the FBSDE system \eqref{eq:FBSDE_canonical} has a unique adapted solution \((X,Y,Z)\in\mathcal S^2\times\mathcal S^2\times\mathcal H^2\).
\end{theorem}

\begin{leanproofbox}{Proof Sketch of Theorem \ref{thm:PP_decoupled}}
The proof proceeds in two distinct, sequential steps, leveraging the decoupled structure.
\begin{enumerate}
    \item \textbf{Solve the Forward SDE:} The forward equation \(dX_t = \bm\mu(t,X_t)dt + \bm\sigma(t,X_t)dW_t\) is a standard SDE. Under the Lipschitz and linear growth conditions of \ref{H1}-\ref{H2}, classical SDE theory (see, e.g., Karatzas and Shreve \cite{KaratzasShreve1991}) guarantees the existence of a unique strong solution \(X \in \mathcal S^2\). This solution is constructed via Picard iteration on the integral form of the SDE.
    \item \textbf{Solve the Backward SDE:} With the path of \(X\) now fixed and known, the driver becomes a function of time and the backward variables: \(\tilde{f}(t,y,z) \coloneqq f(t,X_t(\omega),y,z)\). This function is still Lipschitz in \((y,z)\) for almost every \(\omega\). The terminal condition is the random variable \(g(X_T(\omega))\). The seminal result of Pardoux and Peng (1990) \cite{PardouxPeng1990} ensures that this standard (non-forward-coupled) BSDE has a unique solution pair \((Y,Z) \in \mathcal S^2 \times \mathcal H^2\). The proof of this latter result itself relies on constructing a contraction mapping on the space \(\mathcal{H}^2\). For any given process \(Z \in \mathcal{H}^2\), one can define a corresponding \(Y\) process via the martingale representation theorem. This defines a map from \(\mathcal{H}^2\) to itself, which can be shown to be a contraction under the \(\mathcal{H}^2\) norm. The Banach fixed-point theorem then yields a unique fixed point for \(Z\), which in turn defines a unique \(Y\).
\end{enumerate}
A priori estimates derived using It么's formula and Gr枚nwall's inequality confirm that the solution remains within the required \(L^2\) spaces, completing the proof.
\end{leanproofbox}

\begin{proposition}[Nonlinear Feynman-Kac Formula]
\label{prop:feynman_kac}
In the setting of Theorem \ref{thm:PP_decoupled}, if the coefficients are sufficiently smooth, the solution pair \((Y,Z)\) can be represented by a deterministic function \(u: [0,T] \times \mathbb{R}^d \to \mathbb{R}^q\) such that \(Y_t = u(t,X_t)\) and \(Z_t = \nabla_x u(t,X_t)^\top \bm\sigma(t,X_t)\). This function \(u(t,x)\) is the unique classical (or viscosity) solution to the semilinear parabolic PDE:
\[
\partial_t u + \mathcal L u + f(t,x,u,\nabla_x u^\top \bm\sigma)=0, \quad u(T,x) = g(x),
\]
where \(\mathcal{L}u = \frac{1}{2}\Tr[\bm\sigma\bm\sigma^\top \mathrm{Hess}_x u] + \bm\mu \cdot \nabla_x u\) is the infinitesimal generator of \(X\).
\end{proposition}

\begin{remark}[On Viscosity Solutions]
The term "solution" for the PDE must often be interpreted in a weak sense, typically as a \emph{viscosity solution}. This notion, developed by Crandall, Ishii, and Lions, is crucial because the value function \(u(t,x)\) from a stochastic control problem is often not twice continuously differentiable (\(C^{1,2}\)), even if the underlying coefficients are smooth. The value function may have "kinks" where the optimal policy changes. A viscosity solution does not need to be differentiable everywhere; instead, it must satisfy the PDE inequality sense whenever it is touched from above or below by a smooth "test function". The Feynman-Kac formula provides a probabilistic representation that holds for viscosity solutions, bypassing the need for classical smoothness and broadening the applicability of the PDE connection. The curse of dimensionality makes solving this PDE directly intractable for $d \gg 3$, which is the primary motivation for developing probabilistic numerical methods like the ones in this manuscript.
\end{remark}

\subsection{Fully Coupled Systems: The Monotone Case}
\label{sec:fully_coupled}

When the forward coefficients depend on \((Y,Z)\), the system is **fully coupled**. This feedback loop makes the analysis significantly more challenging. Simple Picard iteration is no longer sufficient, and a stronger condition is needed.

\begin{theorem}[Existence and Uniqueness for Monotone Coupled FBSDEs]
\label{thm:YongMa}
Under the full set of hypotheses \ref{H1}-\ref{H3}, if the time horizon \(T\) is sufficiently small, the fully coupled FBSDE system \eqref{eq:FBSDE_canonical} admits a unique adapted solution \((X,Y,Z) \in \mathcal{S}^2 \times \mathcal{S}^2 \times \mathcal{H}^2\). If the monotonicity condition \ref{H3} is strict (\(\alpha > 0\)), the result holds for an arbitrary time horizon \(T\).
\end{theorem}

\begin{leanproofbox}{Idea of the proof for Theorem \ref{thm:YongMa}}
The proof relies on a fixed-point argument on a carefully chosen space.
\begin{enumerate}
    \item \textbf{Define the Mapping:} Construct a map \(\Phi\) on the solution space \(\mathcal{S}^2 \times \mathcal{H}^2\). For a given pair of processes \((\bar{Y}, \bar{Z})\), \(\Phi\) produces a new pair \((Y,Z)\) by first solving the forward SDE with \((\bar{Y}, \bar{Z})\) plugged into the coefficients, and then solving the resulting BSDE. A solution to the original FBSDE is a fixed point of this map.
    \item \textbf{Show Contraction:} The core of the proof is to show that \(\Phi\) is a contraction mapping. This is where the monotonicity condition \ref{H3} is essential. Consider two inputs \((\bar{Y}^1, \bar{Z}^1)\) and \((\bar{Y}^2, \bar{Z}^2)\) and their corresponding outputs. Let \(\delta Y = Y^1 - Y^2\), etc. Applying It么's formula to \(e^{\beta t}\|\delta Y_t\|^2\) for a well-chosen \(\beta > 0\) and using the monotonicity condition \ref{H3} leads to an inequality that bounds the norm of the output difference by a factor times the norm of the input difference. The monotonicity provides a crucial negative term \(-\alpha \|\delta Y_t\|^2\) which helps absorb other terms.
    \item \textbf{Fixed Point:} For a sufficiently small time horizon \(T\), the Lipschitz properties of the coefficients ensure the map is a contraction. The Banach fixed-point theorem then guarantees the existence of a unique solution. If monotonicity is strict (\(\alpha > 0\)), the negative term it introduces can be used to make the map a contraction for any \(T\) by choosing \(\beta\) large enough. This local solution can be extended to the full interval \([0,T]\) by a stitching argument if needed.
\end{enumerate}
The full details can be found in Ma and Yong (1999) \cite{MaYong1999}.
\end{leanproofbox}

\begin{remark}[On Non-Monotone Systems]
When the monotonicity condition \ref{H3} does not hold, global existence is not guaranteed. However, it is often possible to prove local existence on a small time interval \([T-\delta, T]\) using Picard iterations, provided the Lipschitz constants are not too large. This is a common starting point for analyzing systems that arise in economics, where strict monotonicity may not hold globally.
\end{remark}

\subsection{Malliavin Differentiability and the Clark-Ocone Formula}
\label{sec:malliavin_clark_ocone}

When the coefficients are sufficiently smooth, the solution processes are differentiable in the sense of Malliavin calculus. This provides a powerful connection between the control process \(Z_t\) and the Malliavin derivative of the terminal value.

\begin{definition}[Malliavin Derivative]
The Malliavin derivative, \(D_t F\), of a random variable \(F\) that depends on the entire path of the Brownian motion \(W\), measures the sensitivity of \(F\) to an infinitesimal perturbation of the path of \(W\) at time \(t\). It can be thought of as a form of functional derivative with respect to the Brownian path.
\end{definition}

\begin{proposition}[Clark-Ocone Formula for BSDEs]
\label{prop:clark_ocone}
Assume the coefficients of the FBSDE system are continuously differentiable with bounded derivatives. Then the solution \((Y,Z)\) is Malliavin differentiable. The control process \(Z_t\) admits the representation:
\[
  Z_t = \mathbb E\!\left[
      D_t Y_T
      \,\Big|\,\mathcal F_t\right],
\]
where \(D_t Y_T\) is the Malliavin derivative of the terminal value \(Y_T\). Expanding \(Y_T = g(X_T)\) and the BSDE dynamics, this becomes:
\[
  Z_t = \mathbb E\!\left[
      D_t g(X_T)
      +\int_t^T D_t f\bigl(s,X_s,Y_s,Z_s\bigr) ds
      \,\Big|\,\mathcal F_t\right].
\]
\end{proposition}

\begin{remark}[Significance for Numerics]
The Clark-Ocone formula is more than a theoretical curiosity; it is the foundation for some of the most advanced numerical methods. It reveals that \(Z_t\) is not an arbitrary control but is intrinsically linked to the sensitivity of the terminal outcome to perturbations in the Brownian path at time \(t\). This relationship is exploited by the Malliavin-AD Tower (Section \ref{sec:malliavin_tower_rigorous}) to create highly stable and low-variance estimators for \(Z_t\), which is particularly crucial in high-dimensional settings. The naive estimator for \(Z_t\) from an Euler scheme has variance that blows up as the time step goes to zero, whereas Malliavin-based estimators can have bounded or even vanishing variance. For a full treatment, see Nualart \cite{Nualart2006}.
\end{remark}

\clearpage
\newpage
\part{Numerical Algorithms for FBSDEs}
\label{part:numerics}

\begin{center}
\textit{Analysis tells us a solution exists; numerics lets us see it.}
\end{center}

This part transitions from theory to practice, developing the numerical machinery required to solve the FBSDE system \eqref{eq:FBSDE_canonical}. We build a hierarchy of methods, starting with classical time-stepping schemes, moving to variance-reduction techniques, and culminating in the family of deep learning solvers that have revolutionized the field by breaking the curse of dimensionality.

\bigskip
\hrule
\bigskip

\section{Discretization Preliminaries}
\label{sec:discrete_notation}

\begin{definition}[Time Discretization]
Let \(N \ge 1\) be the number of time steps. We consider a uniform time grid \(0=t_0<t_1<\dots<t_N=T\) with step size \(h = T/N\). The increment of the Brownian motion over the \(k\)-th interval is denoted by \(\Delta W_k \coloneqq W_{t_{k+1}}-W_{t_k}\), which are independent random vectors with distribution \(\mathcal N(0,h\,I_m)\). We use \(\mathsf{X}_k, \mathsf{Y}_k, \mathsf{Z}_k\) to denote the numerical approximations of the true processes \(X_{t_k}, Y_{t_k}, Z_{t_k}\).
\end{definition}

\begin{definition}[Error Metrics]
We assess the quality of our numerical schemes using two standard error metrics:
\begin{itemize}
    \item \textbf{Strong Error:} Measures the pathwise deviation of the numerical solution from the true solution. It is crucial for applications like hedging where the actual path matters.
    \[
      \epsilon_{\mathrm{strong}} \coloneqq \max_{k}\, \left( \mathbb{E}\left[ \|X_{t_k}-\mathsf{X}_k\|^2 \right] \right)^{1/2}.
    \]
    \item \textbf{Weak Error:} Measures the error in the expectation of a smooth function of the solution. This is the relevant metric for most pricing applications, where one is interested in the expected value of a payoff.
    \[
      \epsilon_{\mathrm{weak}} \coloneqq \max_{k}\, \left| \mathbb{E}\left[\phi(X_{t_k})\right] - \mathbb{E}\left[\phi(\mathsf{X}_k)\right] \right|,
    \]
    for a suitable test function \(\phi\).
\end{itemize}
\end{definition}

\section{Classical Time-Stepping Schemes}
\label{sec:classical}

\subsection{The Euler-Maruyama Scheme and the LSMC Method}
\label{sec:euler_picard}

\begin{definition}[Euler-Maruyama Scheme]
The simplest time-stepping scheme for the FBSDE system \eqref{eq:FBSDE_canonical} is the Euler-Maruyama scheme. Given $(\mathsf{X}_k, \mathsf{Y}_k, \mathsf{Z}_k)$, the next state is approximated as:
\begin{align*}
\mathsf{X}_{k+1} &= \mathsf{X}_k + \bm{\mu}(t_k, \mathsf{X}_k, \mathsf{Y}_k, \mathsf{Z}_k)h + \bm{\sigma}(t_k, \mathsf{X}_k, \mathsf{Y}_k)\Delta W_k, \\
\mathsf{Y}_{k+1} &= \mathsf{Y}_k - f(t_k, \mathsf{X}_k, \mathsf{Y}_k, \mathsf{Z}_k)h + \mathsf{Z}_k \Delta W_k.
\end{align*}
This scheme has strong order 0.5 and weak order 1.0 under sufficient smoothness conditions on the coefficients.
\end{definition}

\begin{proposition}[Discrete BSDE Relation]
The integral form of the BSDE, \(Y_t = Y_T + \int_t^T f(s,\dots)ds - \int_t^T Z_s dW_s\), leads to the discrete-time relation by taking conditional expectations. Integrating from $t_k$ to $t_{k+1}$ and taking conditional expectation at $t_k$ gives:
\[
\mathsf{Y}_k = \mathbb{E}_k[\mathsf{Y}_{k+1}] + f(t_k, \mathsf{X}_k, \mathsf{Y}_k, \mathsf{Z}_k)h + \mathcal{O}(h^{3/2}).
\]
A key insight is that \(Z_k\) is related to the correlation between the future value \(Y_{k+1}\) and the Brownian shock \(\Delta W_k\). Multiplying the BSDE by \(\Delta W_k\) and taking conditional expectations yields the identity for \(\mathsf{Z}_k\):
\[
\mathsf{Z}_k \approx \frac{1}{h}\mathbb{E}_k[\mathsf{Y}_{k+1} \Delta W_k^\top].
\]
\end{proposition}

\begin{remark}[The Curse of Dimensionality in LSMC]
The elegance of these explicit relations hides a major practical difficulty: the computation of the conditional expectations \(\mathbb{E}_k[\cdot]\). In a Markovian setting, these are functions of the state \(\mathsf{X}_k\). The Least-Squares Monte Carlo (LSMC) method, pioneered by Gobet, Lemor, and Warin \cite{Gobet2005}, approximates these functions by regressing simulated future values onto a set of basis functions of \(\mathsf{X}_k\) (e.g., polynomials). This is effective in low dimensions (\(d \le 4\)) but succumbs to the curse of dimensionality, as the number of basis functions required to accurately represent a function in \(d\) dimensions grows exponentially with \(d\). This limitation was the primary motivation for developing the deep learning approaches in Section \ref{sec:deep_solvers}.
\end{remark}

\subsection{Higher-Order Schemes and Bias Reduction}
\label{sec:higher_order_mlmc}

\begin{proposition}[It么-Stratonovich Conversion]
An It么 SDE $dX_t = \mu dt + \sigma dW_t$ can be written in Stratonovich form as $dX_t = \mu^\circ dt + \sigma \circ dW_t$, where the Stratonovich drift $\mu^\circ$ is related to the It么 drift $\mu$ by the It么-Stratonovich correction term:
\[
\mu^\circ_i(t,x) = \mu_i(t,x) - \frac{1}{2} \sum_{j=1}^d \sum_{k=1}^m \frac{\partial \sigma_{ik}}{\partial x_j}(t,x) \sigma_{jk}(t,x).
\]
This conversion is essential for applying Stratonovich-type integrators like the Heun scheme to It么 SDEs, as they are naturally formulated for the Stratonovich integral.
\end{proposition}

\begin{definition}[Heun Scheme for It么 SDEs]
The Heun scheme is a predictor-corrector method. For an It么 SDE, it can be implemented without explicit conversion. It has strong order 0.5 and weak order 1.0.
\begin{enumerate}
    \item \textbf{Predictor (Euler step):} $\tilde{X}_{k+1} = \mathsf{X}_k + \bm{\mu}(t_k, \mathsf{X}_k)h + \bm{\sigma}(t_k, \mathsf{X}_k)\Delta W_k$.
    \item \textbf{Corrector (Trapezoidal rule):} $\mathsf{X}_{k+1} = \mathsf{X}_k + \frac{1}{2}(\bm{\mu}(t_k, \mathsf{X}_k) + \bm{\mu}(t_{k+1}, \tilde{X}_{k+1}))h + \frac{1}{2}(\bm{\sigma}(t_k, \mathsf{X}_k) + \bm{\sigma}(t_{k+1}, \tilde{X}_{k+1}))\Delta W_k$.
\end{enumerate}
\end{definition}

\begin{remark}[Integrator Choice for Deep BSDE Solvers]
While higher weak order is generally desirable, the primary motivation for using the Heun scheme in the context of deep BSDE solvers is its ability to reduce bias in the training objective. As recently formalized by Park \& Tu (2025) \cite{ParkTu2025}, the standard Euler-Maruyama scheme introduces a leading-order bias of size $\mathcal{O}(h)$ into the one-step residual loss function. This "self-consistency bias" arises because the discretization error is correlated with the gradient signal. The symmetric, centered evaluation of the Heun scheme cancels this leading bias term, resulting in a residual bias of $\mathcal{O}(h^2)$. This leads to more accurate and stable training, a point we will leverage in our main application.
\end{remark}

\begin{theorem}[Multi-Level Monte Carlo Complexity, Giles 2008]
\label{thm:mlmc}
MLMC is a variance reduction technique that dramatically improves efficiency for computing expectations. It computes estimates on a hierarchy of grids (from coarse to fine) and combines them to minimize variance for a given computational cost. For a scheme with weak order \(\alpha\) and strong order \(\beta\), the total cost to achieve a root-mean-square error of \(\varepsilon\) with MLMC is:
\[
\text{Cost} \approx \begin{cases} \mathcal{O}(\varepsilon^{-2}) & \text{if } \beta > 1/2 \\ \mathcal{O}(\varepsilon^{-2}(\log\varepsilon)^2) & \text{if } \beta = 1/2 \\ \mathcal{O}(\varepsilon^{-2-(1-2\beta)/\alpha}) & \text{if } \beta < 1/2 \end{cases}
\]
For the Euler scheme (\(\alpha=1, \beta=1/2\)), MLMC achieves nearly optimal \(\mathcal{O}(\varepsilon^{-2})\) complexity, a substantial improvement over the \(\mathcal{O}(\varepsilon^{-3})\) cost of a standard Monte Carlo approach \cite{Giles2008}.
\end{theorem}

\section{Deep Learning Solvers}
\label{sec:deep_solvers}

\subsection{The Vanilla Deep BSDE Algorithm}
\label{sec:deep_vanilla}

\begin{definition}[The Deep BSDE Method, E, Han, Jentzen 2017]
The Deep BSDE method \cite{EHanJentzen2017} reframes the FBSDE problem as a stochastic optimization problem solved forward in time.
\begin{enumerate}
    \item \textbf{Parameterization:} The unknown initial value \(Y_0\) is treated as a trainable parameter. The unknown control process \(Z_t\) is parameterized by a deep neural network, \(\mathsf{Z}_t = \mathcal{N}_\theta(t, X_t)\), which takes the current time and state as input.
    \item \textbf{Forward Simulation:} Starting with an initial guess for \(\mathsf{Y}_0\), the processes \(\mathsf{X}\) and \(\mathsf{Y}\) are simulated forward in time using a numerical scheme (e.g., Euler-Maruyama). At each step \(k\), \(\mathsf{Z}_k\) is computed as \(\mathcal{N}_\theta(t_k, \mathsf{X}_k)\).
    \item \textbf{Loss Function:} The network parameters \(\theta\) and the initial value \(\mathsf{Y}_0\) are trained by minimizing the mean squared error between the terminal value of the simulated process, \(\mathsf{Y}_N\), and the true terminal condition, \(g(\mathsf{X}_N)\).
    \[
      \mathcal{L}(\theta, \mathsf{Y}_0) = \mathbb{E}\left[ \| \mathsf{Y}_N - g(\mathsf{X}_N) \|^2 \right].
    \]
    The expectation is approximated by the sample mean over a batch of simulated paths.
\end{enumerate}
\end{definition}

\begin{remark}[Breaking the Curse of Dimensionality]
The key advantage of this method is that the computational complexity scales polynomially with dimension \(d\), as it only requires sampling paths and evaluating the network, both of which are efficient operations. The universal approximation theorem for neural networks provides theoretical justification that, given sufficient capacity, \(\mathcal{N}_\theta\) can approximate the true function \(Z(t,x)\) arbitrarily well. This makes it feasible to solve problems in hundreds or even thousands of dimensions, a feat impossible for classical grid-based or LSMC methods.
\end{remark}

\subsection{The Malliavin-AD Tower for Stability and Accuracy}
\label{sec:malliavin_tower_rigorous}

\subsubsection{Motivation and Roadmap}
A key weakness of the vanilla Deep BSDE method is that the gradient of the loss with respect to the parameters of the \(Z\) network can have very high variance, as the learning signal comes only from the terminal time. The Malliavin-AD tower is a powerful hierarchy of techniques to mitigate this by enforcing the theoretical structure of the solution at multiple levels.

\begin{proposition}[The Core Idea: Level 0 (Consistency Loss)]
The foundational level of the tower is to augment the vanilla loss with a **consistency loss** that enforces the BSDE structure at every time step.
\[
\mathcal{L}_{\text{tower-0}} = \mathcal{L}_{\text{terminal}} + \lambda_C \sum_{k=0}^{N-1} \mathbb{E}\left[ \| \mathsf{Y}_{k+1} - \Phi_h(\mathsf{Y}_k, \mathsf{Z}_k, \Delta W_k) \|^2 \right],
\]
where \(\Phi_h\) is the one-step numerical integrator for the BSDE. This provides a direct, local learning signal for \(\mathsf{Z}_k\) at each step, dramatically stabilizing training. This is the approach implemented in Appendix \ref{app:GE_impl}.
\end{proposition}

\begin{remark}[The Role of \texttt{stop\_gradient}]
When implementing the consistency loss, it is crucial to apply a \texttt{stop\_gradient} operation to the target value inside the squared norm. For example, if the residual is $\mathcal{R}_k = \mathsf{Y}_{k+1} - \text{target}_{k+1}$, the gradient should only flow through the $\mathsf{Y}_{k+1}$ term. Without this, the optimizer could trivially minimize the loss by learning a degenerate solution where $\mathsf{Y}_{k+1}$ simply equals the target, without learning the correct dynamics.
\end{remark}

\subsubsection{Level-1 Malliavin BSDE}
Higher levels of the tower involve differentiating the original BSDE with respect to the Brownian path to obtain new BSDEs for the Malliavin derivatives.

\begin{definition}[Level-1 Processes]
Fix a reference time \(t\le s\le T\) and a basis vector \(e_i \in \mathbb{R}^m\). Define the Level-1 processes as the Malliavin derivatives of the Level-0 processes \((Y,Z)\):
\[
  U^{i}_{s}\;:=\;D_t^{e_i}Y_{s},
  \qquad
  V^{i}_{s}\;:=\;D_t^{e_i}Z_{s}.
\]
\end{definition}

\begin{proposition}[The Level-1 BSDE]
\label{prop:level1_bsde}
By formally differentiating the original BSDE for \(Y_s\) with respect to the path of \(W\) at time \(t\), the Level-1 processes \((U^i, V^i)\) are found to satisfy a linear BSDE on the interval \([t,T]\):
\begin{equation}\label{eq:U_BSDE_main}
  \mathrm{d} U^{i}_s
  \;=\;
   -\bigl[A_s\,U^{i}_s + B_s\,V^{i}_s + C^{i}_s \bigr]\mathrm{d} s
   \;+\;
   V^{i}_s\,\mathrm{d} W_s,
  \qquad
  U^{i}_T = D_t^{e_i}g(X_T).
\end{equation}
The coefficients are the Jacobians of the original driver \(f\):
\[
  A_s := \partial_y f(s,X_s,Y_s,Z_s), \quad
  B_s := \partial_z f(s,X_s,Y_s,Z_s), \quad
  C^{i}_s := \partial_x f(s,X_s,Y_s,Z_s) \cdot D_t^{e_i}X_s.
\]
\end{proposition}

\begin{remark}[Automatic Differentiation (AD) and Variance Reduction]
In a modern implementation (e.g., using JAX or PyTorch), the Jacobians \(A_s, B_s, C_s\) and the Malliavin derivative of the forward process \(D_t^{e_i}X_s\) can all be computed efficiently and exactly (up to machine precision) using automatic differentiation, hence the name "Malliavin-AD" tower. The naive Euler estimator for \(Z_k\) has variance that scales as \(\mathcal{O}(h^{-1})\). Estimators derived from the Malliavin-AD tower, such as those in the "One-Step Malliavin" schemes, have variance that is stable or even vanishes as \(h \to 0\) \cite{DomelevoWarin2023}. This dramatic reduction in variance is the primary reason for the tower's effectiveness in stabilizing training and achieving high accuracy.
\end{remark}

\subsection{Alternative Deep Learning Architectures}
While the Deep BSDE method is foundational, several alternatives have been proposed to address its limitations, such as high variance or difficulty with stiff drivers.
\begin{itemize}
    \item \textbf{Deep Splitting Method \cite{Beck2019}:} This method combines the probabilistic representation with a deterministic one-step PDE solve. It splits the problem into a local PDE solve over a small time step (often using a finite difference scheme), followed by a Monte Carlo step to handle the expectation. This can be more robust for stiff or highly nonlinear problems where the local dynamics are better captured by a PDE solver.
    \item \textbf{Robust Deep FBSDE Method \cite{Goudenegge2020}:} This approach reformulates the problem to directly learn the function $u(t,x)$ and its gradient \(\nabla_x u(t,x)\) using two separate neural networks. By enforcing the relationship \(Z_t = \nabla_x u(t,X_t)^\top \sigma(t,X_t)\) within the loss function, it often leads to more stable training and provides theoretical convergence guarantees under certain conditions.
    \item \textbf{Deep Backward Dynamic Programming (DBDP) \cite{Hure2020}:} This method more closely resembles classical dynamic programming. It works backward in time, learning the value function at each time step \(t_k\) by regressing on simulated future values at \(t_{k+1}\), similar to LSMC but with neural networks as powerful, high-dimensional function approximators.
\end{itemize}
The choice of method often depends on the specific structure of the problem, such as the degree of nonlinearity, stiffness, and the desired trade-off between computational cost and accuracy. For the application in this paper, the consistency-regularized Deep BSDE method with a Heun integrator provides a robust and efficient solution.

\subsection{A-Posteriori Error Bounds}
\begin{theorem}[A-Posteriori Bound, Bender \& Steiner 2012]
Let \(\mathcal{R}_t\) be the residual process measuring how well the learned solution \((\mathsf{Y}, \mathsf{Z})\) satisfies the BSDE, i.e., \(\mathcal{R}_t = \mathsf{Y}_t - \left( g(X_T) + \int_t^T f(s, X_s, \mathsf{Y}_s, \mathsf{Z}_s) ds - \int_t^T \mathsf{Z}_s dW_s \right)\). Under suitable assumptions on the coefficients, there exists a constant \(C\) such that:
\[
  \|u(0,\xi)-\mathsf{Y}_0\| \le C \left( \mathbb{E}\left[\sup_{t\in [0,T]}\|\mathcal{R}_t\|^2\right] \right)^{1/2}.
\]
This is a powerful tool, as the right-hand side can be estimated from the training data (it is closely related to the consistency loss), providing a way to certify the accuracy of the obtained solution and a principled stopping criterion for training \cite{BenderSteiner2012}.
\end{theorem}

\clearpage
\newpage
\part{A Suite of Benchmark Problems}
\label{part:benchmarks}

\begin{center}
\textit{A new method is only as credible as the old problems it can solve.}
\end{center}

Before applying our numerical framework to a novel economic problem, it is imperative to validate it against a suite of well-understood benchmark cases. This part introduces three canonical problems from mathematical physics and finance that have become standard tests for high-dimensional PDE and FBSDE solvers. For each, we specify the governing equation, its FBSDE representation, and its significance as a benchmark. The explicit coefficient functions are detailed in Appendix \ref{app:benchmark_details}.

\bigskip
\hrule
\bigskip

\section{The Allen-Cahn Equation}
\label{sec:allen_cahn}

\begin{definition}[Allen-Cahn PDE]
The Allen-Cahn equation is a semilinear heat equation that models phase separation in materials science. Its solution \(u(t,x)\) satisfies:
\begin{equation}
    \partial_t u(t,x) + \frac{1}{2}\Delta u(t,x) + u(t,x) - u(t,x)^3 = 0,
\end{equation}
on \([0,T] \times \mathbb{R}^d\), with a terminal condition \(u(T,x) = g(x)\).
\end{definition}

\begin{proposition}[FBSDE Representation of Allen-Cahn]
By the nonlinear Feynman-Kac formula (Proposition \ref{prop:feynman_kac}), the solution \(u(t,x)\) corresponds to the \(Y\) component of a decoupled FBSDE system where:
\begin{itemize}
    \item \textbf{Forward SDE:} \(dX_t = dW_t\), with \(X_t=x\). This is a standard \(d\)-dimensional Brownian motion.
    \item \textbf{Backward SDE:} \(dY_t = -(Y_t - Y_t^3)dt + Z_t dW_t\), with \(Y_T = g(X_T)\).
\end{itemize}
\end{proposition}

\begin{remark}[Significance as a Benchmark]
The Allen-Cahn equation is an excellent benchmark for several reasons:
\begin{enumerate}
    \item \textbf{High-Dimensionality:} It is straightforward to extend to any dimension \(d\), making it a standard test for the curse of dimensionality. The 100-dimensional version was the flagship example in the original Deep BSDE paper \cite{EHanJentzen2017}.
    \item \textbf{Nonlinearity:} The cubic term \(u-u^3\) introduces a significant nonlinearity that challenges many classical numerical methods.
    \item \textbf{Known Behavior:} While a closed-form solution is generally unavailable, the qualitative behavior (phase separation) is well-understood, and for specific choices of \(g(x)\), approximate or asymptotic solutions exist for comparison.
\end{enumerate}
\end{remark}

\section{The Hamilton-Jacobi-Bellman Equation}
\label{sec:hjb}

\begin{definition}[HJB Equation for Optimal Control]
The Hamilton-Jacobi-Bellman (HJB) equation arises from stochastic optimal control. A typical example involves controlling a process \(X_t\) to minimize a cost. The value function \(u(t,x)\) solves:
\begin{equation}
    \partial_t u(t,x) + \inf_{\alpha_t \in \mathcal{A}} \left\{ \mathcal{L}^{\alpha_t} u(t,x) + C(t,x,\alpha_t) \right\} = 0,
\end{equation}
where \(\mathcal{L}^{\alpha_t}\) is the generator of \(X_t\) under control \(\alpha_t\), and \(C\) is the running cost. A common case is controlling the drift, leading to a quadratic nonlinearity in the gradient of \(u\).
\end{definition}

\begin{proposition}[FBSDE Representation of HJB]
For many standard control problems, the HJB equation can be linked to an FBSDE. For a drift control problem with quadratic running cost, the optimal control is often a linear function of the gradient, \(\alpha_t^* = -K \nabla_x u(t,X_t)\). Substituting this back into the HJB equation yields a semilinear PDE. The corresponding FBSDE has:
\begin{itemize}
    \item \textbf{Forward SDE:} \(dX_t = \alpha_t^* dt + \sigma dW_t\). Note that \(\alpha_t^*\) depends on \(\nabla_x u\), which is related to \(Z_t\). This can lead to a coupled system.
    \item \textbf{Backward SDE:} The driver \(f\) will contain terms related to the running cost \(C\) and the optimal control. This driver often has quadratic growth in $Z$, requiring the theory of Quadratic BSDEs.
\end{itemize}
\end{proposition}

\begin{remark}[Significance as a Benchmark]
HJB equations are a cornerstone of dynamic optimization in economics and engineering.
\begin{enumerate}
    \item \textbf{Economic Relevance:} They directly model decision-making under uncertainty.
    \item \textbf{Challenging Nonlinearity:} The Hamiltonian term often involves a quadratic dependence on \(\nabla_x u\), which translates to a quadratic dependence on \(Z\) in the BSDE driver. This makes it an excellent test case for the QBSDE theory and numerical stabilization techniques discussed in Section \ref{subsubsec:GE_QBSDE_Wellposedness}.
    \item \textbf{Verification:} For Linear-Quadratic (LQ) control problems, the HJB equation reduces to a set of Riccati ODEs, providing a rare case where a high-dimensional (\(d>1\)) solution is known in closed form, making it an invaluable test for accuracy.
\end{enumerate}
\end{remark}

\section{Black-Scholes Equation with Nonlinearities}
\label{sec:nonlinear_bs}

\begin{definition}[Nonlinear Black-Scholes PDE]
The classical Black-Scholes model for option pricing leads to a linear PDE. However, realistic extensions incorporating market frictions like transaction costs, imperfect hedging, or differential funding costs for borrowing and lending introduce nonlinearities. A common example is a model with funding costs that depend on the size and sign of the hedging portfolio, leading to a PDE of the form:
\begin{equation}
    \partial_t u + r x \partial_x u + \frac{1}{2}\sigma^2 x^2 \partial_{xx}^2 u - R(u - x\partial_x u) = 0,
\end{equation}
where \(R\) is a nonlinear funding cost function.
\end{definition}

\begin{proposition}[BSDE Representation]
This nonlinear PDE corresponds to a BSDE where the driver \(f\) depends on both \(Y\) and \(Z\).
\begin{itemize}
    \item \textbf{Forward SDE:} The underlying asset price follows a GBM under the risk-neutral measure: \(dX_t = r X_t dt + \sigma X_t dW_t\).
    \item \textbf{Backward SDE:} The option price \(Y_t = u(t,X_t)\) and hedge \(\Delta_t = \partial_x u(t,X_t)\) are related by \(Z_t = \Delta_t \sigma X_t\). The BSDE is:
    \[ dY_t = -(-r Y_t + R(Y_t - Z_t/\sigma))dt + Z_t dW_t. \]
\end{itemize}
\end{proposition}

\begin{remark}[Significance as a Benchmark]
This class of problems is critical for testing financial applications.
\begin{enumerate}
    \item \textbf{Practical Importance:} It models a core problem in modern quantitative financepricing and hedging in the presence of market frictions.
    \item \textbf{Financial Intuition:} The variables \(Y_t\) and \(Z_t\) have direct financial interpretations as the price and the volatility-scaled delta-hedge of the derivative, respectively. This allows for sanity checks based on financial principles (e.g., put-call parity, monotonicity of prices).
    \item \textbf{Comparison to Linear Case:} The solution can be directly compared to the classical Black-Scholes solution, allowing for a precise quantification of the impact of the nonlinearity.
\end{enumerate}
\end{remark}

\clearpage
\newpage
\part{Application: Log-Utility Investor with Two Lucas Trees}
\label{part:finance}

\begin{center}
\textit{"All theory, dear friend, is grey; but green is life's golden tree."}\\
\textemdash\,Goethe, \emph{Faust}
\end{center}

This part shows how the forward-backward SDE framework can be deployed in a setting that is analytically transparent and yet rich enough to stress numerical solvers: a representative-log-utility investor who trades two Lucas trees. The two trees deliver distinct dividend processes driven by correlated shocks. Because the representative agent has log utility, the equilibrium can be characterized in closed form, which allows us to write down an exact FBSDE system for the price-consumption ratios of each tree. That closed-form structure makes the model an ideal regression test for our multicountry toolkit: every equation can be mapped directly to the primitives in `Tex/Model.tex`, the stochastic discount factor is observable, and the quadratic terms in the driver arise solely from the diffusion matrix linking the two dividend shocks.

\bigskip
\hrule
\bigskip

\section{Economic Environment and Equilibrium Mapping}
\label{sec:two_tree_environment}

\begin{definition}[Economic Primitives]
We work on a filtered probability space $(\Omega,\mathcal F,(\mathcal F_t)_{t\ge 0},\mathbb P)$ supporting a two-dimensional Brownian motion $(W^1_t,W^2_t)^\top$. Let $\bm{D}_t = (D^1_t,D^2_t)^\top$ denote the dividend vector delivered by the two Lucas trees. Each component follows a correlated geometric Brownian motion:
\begin{equation}
\label{eq:two_tree_dividends}
  \frac{dD^j_t}{D^j_t} = \mu_j\,dt + \bm{\sigma}_j^\top d\bm{W}_t, \quad j\in\{1,2\},
\end{equation}
with constant drifts $\bm{\mu}=(\mu_1,\mu_2)^\top$ and diffusion rows $\bm{\sigma}_j^\top$, collected in the full-rank matrix $\bm{\Sigma} = [\bm{\sigma}_1,\bm{\sigma}_2]^\top$. The instantaneous covariance matrix is $\bm{\Omega} = \bm{\Sigma}\bm{\Sigma}^\top$. The representative investor has log utility and discounts the future at rate $\rho>0$:
\begin{equation}
\label{eq:log_utility_preferences}
  U(c) = \mathbb{E}\Bigl[ \int_0^\infty e^{-\rho t} \log c_t\,dt \Bigr].
\end{equation}
Markets are complete: the agent trades the two tree claims $(S^1_t,S^2_t)$ (unit supply each) and a risk-free asset with instantaneous rate $\{r_t\}$.
\end{definition}

\begin{remark}[Two-Tree Cash-Flow Structure]
The total consumption good is $C_t = D^1_t + D^2_t$. Let $\lambda^j_t = D^j_t/C_t$ denote the share of consumption delivered by tree $j$. Its diffusion is inherited from \eqref{eq:two_tree_dividends} and ensures $\lambda^1_t + \lambda^2_t = 1$ at all times. We denote $\bm{\lambda}_t = (\lambda^1_t,\lambda^2_t)^\top$.
\end{remark}

\section{Closed-Form Equilibrium and the Stochastic Discount Factor}
\label{sec:two_tree_equilibrium}

\begin{proposition}[Representative-Agent Allocation]
\label{prop:log_allocation}
In equilibrium the agent consumes the aggregate endowment, $C_t = D^1_t + D^2_t$, and holds the entire supply of each tree. The stochastic discount factor is
\begin{equation}
\label{eq:two_tree_sdf}
  \xi_t = \frac{e^{-\rho t}}{C_t}.
\end{equation}
The aggregate consumption growth satisfies
\begin{equation}
\label{eq:consumption_growth_drift}
  \frac{dC_t}{C_t} = g_{C,t}\,dt + \bm{\sigma}_{C,t}^\top d\bm{W}_t,
\end{equation}
with drift $g_{C,t} = \bm{\lambda}_t^\top \bm{\mu}$ and diffusion row $\bm{\sigma}_{C,t} = \bm{\lambda}_t^\top \bm{\Sigma}$. The short rate and market price of risk implied by \eqref{eq:two_tree_sdf} are
\begin{equation}
\label{eq:two_tree_short_rate}
  r_t = \rho + g_{C,t} - \frac{1}{2} \| \bm{\sigma}_{C,t} \|^2,
\qquad
  \bm{\kappa}_t = \bm{\sigma}_{C,t}.
\end{equation}
\end{proposition}

\begin{leanproofbox}{Proof Sketch of Proposition \ref{prop:log_allocation}}
Log utility implies consumption equals wealth at rate $\rho$ (Merton, 1971). Market clearing pins consumption to the sum of dividends. Applying It\^o's lemma to \eqref{eq:two_tree_sdf} yields $(d\xi_t/\xi_t = -(r_t\,dt + \bm{\kappa}_t^\top d\bm{W}_t))$ with $r_t$ and $\bm{\kappa}_t$ as stated once we substitute \eqref{eq:consumption_growth_drift}.
\end{leanproofbox}

\begin{theorem}[Two-Tree Price-Consumption Ratios as a BSDE]
\label{thm:two_tree_bsde}
Let $\bm{q}_t = (q^1_t,q^2_t)^\top$ with $q^j_t = S^j_t / C_t$. Then $(\bm{X}_t, \bm{q}_t, \bm{Z}_t)$ solves the Markovian BSDE system
\begin{align}
  d\bm{X}_t &= \bm{b}(\bm{X}_t)\,dt + \bm{\Sigma}\,d\bm{W}_t, \label{eq:two_tree_forward} \\
  dq^j_t &= -f^j(\bm{X}_t, \bm{q}_t, \bm{z}^j_t)\,dt + (\bm{z}^j_t)^\top d\bm{W}_t, \qquad j=1,2, \label{eq:two_tree_backward} \\
  \lim_{T\to\infty} e^{-\rho T} q^j_T &= 0, \label{eq:two_tree_terminal}
\end{align}
where the state is $\bm{X}_t = (\log D^1_t, \log D^2_t)^\top$ and the driver is
\begin{equation}
\label{eq:two_tree_driver}
  f^j(\bm{X}, \bm{q}, \bm{z}^j) = \lambda^j(\bm{X}) - ( r(\bm{X}) - g_C(\bm{X}) ) q^j + \bm{z}^{j\top} \bm{\sigma}_C(\bm{X}),
\end{equation}
with $\lambda^j(\bm{X}) = e^{x_j}/(e^{x_1} + e^{x_2})$, $\bm{\sigma}_C(\bm{X}) = \lambda^1(\bm{X}) \bm{\sigma}_1 + \lambda^2(\bm{X}) \bm{\sigma}_2$, and $r(\bm{X})$, $g_C(\bm{X})$ given by \eqref{eq:two_tree_short_rate}.
\end{theorem}

\begin{leanproofbox}{Proof Sketch of Theorem \ref{thm:two_tree_bsde}}
Start from the valuation identity $S^j_t = \mathbb{E}_t[\int_t^{\infty} (\xi_s/\xi_t) D^j_s ds]$. Differentiate $S^j_t = q^j_t C_t$ using It\^o and substitute the return dynamics implied by \eqref{eq:two_tree_sdf}. Terms proportional to $d\bm{W}_t$ yield the martingale component $\bm{z}^j_t$, while the drift terms deliver \eqref{eq:two_tree_driver}. The transversality condition \eqref{eq:two_tree_terminal} follows from the log-utility no-bubble condition.
\end{leanproofbox}

\section{Numerical Formulation}
\label{sec:two_tree_numerics}

\subsection{State Transformation and Control Dimensions}

\begin{definition}[State System for the Solver]
The forward state used in our SolverND implementation is $\bm{X}_t = (x^1_t, x^2_t)^\top = (\log D^1_t, \log D^2_t)^\top$. Its drift is $\bm{b}(\bm{X}) = \bm{\mu} - \tfrac{1}{2}\operatorname{diag}(\bm{\Sigma}\bm{\Sigma}^\top)$, which follows from It\^o's lemma applied to \eqref{eq:two_tree_dividends}. The control process for each backward component has the same dimension as the Brownian motion: $\bm{z}^j_t\in\mathbb{R}^2$.
\end{definition}

\begin{proposition}[Structure of the Driver]
\label{prop:two_tree_driver_structure}
The driver \eqref{eq:two_tree_driver} is affine in $\bm{z}^j$ and inherits quadratic growth through the dependence of $\bm{\sigma}_C(\bm{X})$ on the exponentials of the state variables. Consequently, the BSDE is diagonally quadratic and satisfies the assumptions required by the SolverND routines introduced in Section \ref{sec:solver_nd}.
\end{proposition}

\begin{remark}[Terminal Condition Handling]
In practice we implement \eqref{eq:two_tree_terminal} by simulating the system on a large but finite horizon $[0,T]$ and enforcing $q^j_T = 0$ as the terminal condition. The choice of $T$ is handled exactly as in the multicountry notebook: set \texttt{NOTEBOOK_FAST} to trigger short horizons for smoke tests and use the full horizon for production replications.
\end{remark}

\subsection{Calibration and Diagnostics}

\begin{itemize}[leftmargin=1.2em]
  \item \textbf{Calibration Loader:} The JSON entry `two_tree` supplies $\bm{\mu}$, $\bm{\Sigma}$, and $\rho$. We reuse the validation utilities from Section \ref{sec:calibration_tools} to ensure positive definiteness of $\bm{\Omega}$.
  \item \textbf{Moment Targets:} The primary statistics are the steady-state price-consumption ratios $q^j$ and the covariance matrix of returns. Because the solution is analytic, the CLI `scripts/compare_table1_solver.py --from-tex` can check equality to machine precision.
  \item \textbf{Animation Hooks:} Rolling correlations and impulse responses now operate on the two dividend growth rates instead of wealth shares. The primitives notebook contains a QQ-plot diagnostic for the bivariate Gaussian driver implied by $\bm{\Sigma}$.
\end{itemize}

\section{Research Takeaways}
\label{sec:two_tree_takeaways}

\begin{itemize}[leftmargin=1.2em]
  \item \textbf{Analytic Benchmark:} The log-utility structure converts the two-tree economy into a closed-form BSDE, providing a stringent unit test for SolverND and MacroFinanceNet without appealing to numerical approximations.
  \item \textbf{Stress on Diffusion Coupling:} Correlated dividend shocks enter exclusively through $\bm{\sigma}_C(\bm{X})$, making it easy to isolate matrix-diffusion bugs.
  \item \textbf{Gateway to Multicountry Model:} The mapping in Section \ref{sec:two_tree_equilibrium} carries over to the multicountry setting once country-level trees are stacked. The same moment utilities and LaTeX extractors feed the Table~1 replication pipeline.
\end{itemize}
\part{Extensions and Outlook}
\label{part:extensions}

\begin{center}
\textit{The reward for good work is more work.}
\end{center}

This final part provides a brief survey of several active research frontiers, highlighting current challenges and future opportunities that extend beyond the canonical Markovian, Brownian-driven setting.

\bigskip\hrule\bigskip

\section{Delay and Path-Dependent FBSDEs}
\label{sec:path_dependence}

\paragraph{Motivation.} Many real-world systems exhibit memory. Financial models with transaction costs, macroeconomic models with implementation lags, and reinforcement learning with delayed rewards all lead to dynamics where the coefficients depend on the entire past history of the processes, \(X_{[0,t]}\).

\paragraph{Theory and Numerics.} The analysis of such systems requires the tools of functional It么 calculus. Well-posedness has been established under appropriate Lipschitz conditions on path space \cite{ContFournie2013}. Numerically, this presents a challenge for standard MLP-based deep solvers. Architectures with memory, such as LSTMs, Transformers, or Neural CDEs, are natural candidates for parameterizing the control process \(Z_t\) and have shown promising results.

\section{Jumps and L茅vy Processes}
\label{sec:jumps}

\paragraph{Motivation.} Brownian motion is not always sufficient to capture the sudden, discontinuous events seen in finance (market crashes, defaults) or insurance (catastrophic claims). Incorporating jump processes (like Poisson or more general L茅vy processes) leads to Partial Integro-Differential Equations (PIDEs) and their probabilistic counterparts, FBSDEs with jumps.

\paragraph{Theory and Numerics.} The BSDE component is augmented with an additional integral with respect to a compensated jump measure. Existence and uniqueness results exist for certain classes of these systems \cite{BarlesBuckdahnPardoux1997}. Numerically, deep learning solvers must be extended to learn the additional jump-related control variable. This involves sampling jump times and sizes during the forward simulation and adds another layer of complexity to the training process. Variance control for the jump component is a key challenge \cite{Bachouch2022}.

\section{Mean-Field Games and FBSDEs}
\label{sec:meanfield}

\paragraph{Motivation.} How does one model a system with a very large number of interacting agents, like a flock of birds, a crowd of pedestrians, or a market with thousands of traders? Mean-field game theory provides a framework by considering the limit as the number of agents goes to infinity. Each agent interacts not with every other agent, but with the statistical distribution (the "mean field") of the entire population.

\paragraph{Theory and Numerics.} The equilibrium in a mean-field game is characterized by a coupled system consisting of a forward HJB equation (for the individual agent's control) and a backward Fokker-Planck equation (for the evolution of the population distribution). This system can be represented as a mean-field FBSDE, where the coefficients depend on the law of the solution \cite{Carmona2016}. Numerically, this is extremely challenging. "Learning-by-particles" methods, where the deep solver is trained on a large ensemble of interacting particles, are the current state-of-the-art. For such systems, architectures that respect permutation symmetry, such as Graph Neural Networks (GNNs), are essential for creating scalable and structurally sound models \cite{MAGNet2019, GMADDPG2023}.

\section{Grand Challenges for 2030}
\label{sec:challenges}

The confluence of probability theory, numerical analysis, and machine learning has opened up exciting new possibilities. Looking ahead, we identify several "grand challenges" that could define the next decade of research in this field:

\begin{enumerate}[label=\textbf{C\arabic*}. ,leftmargin=1.6em]
  \item \textbf{Scaling and Certification:} Solve a 1,000-dimensional, fully coupled FBSDE with a certified error of less than \(10^{-3}\) in under 24 hours of computation. This requires breakthroughs in both algorithmic efficiency (e.g., adaptive sampling, better network architectures) and theoretical understanding (tighter a-posteriori error bounds).
  \item \textbf{Real-Time Control:} Develop pre-trained FBSDE surrogates that can compute optimal hedging strategies (Deltas and Gammas) for complex financial derivatives in real-time. This involves moving from offline training to online inference, potentially using techniques like neural operator learning.
  \item \textbf{Data-Driven Economics:} Solve and calibrate a mean-field game model with \(10^4\) heterogeneous agents to real economic data, providing a new generation of macroeconomic models. This requires tackling the joint challenge of high-dimensional solution and statistical estimation (the "inverse problem"), including issues of non-stationarity and model misspecification.
  \item \textbf{Formal Verification:} Develop methods, possibly leveraging techniques from neural theorem proving, to formally verify that a trained neural network solver respects fundamental theoretical properties like the comparison principle for BSDEs or no-arbitrage constraints in finance. This would be a major step towards building trust in these black-box models for high-stakes applications.
\end{enumerate}

\bigskip\hrule\bigskip

\section*{Epilogue}
\addcontentsline{toc}{section}{Epilogue}

The theory of FBSDEs has journeyed from an abstract mathematical curiosity to a powerful computational tool in less than a century. The path from Kolmogorovs diffusions to todays GPU-accelerated deep solvers illustrates a beautiful interplay between theoretical insight and computational innovation. The next breakthroughs will undoubtedly require even deeper collaboration across disciplines. With open-source benchmarks, certifiable algorithms, and ever-more-powerful hardware, forward-backward stochastic reasoning is poised to become an indispensable part of the modern scientist's and engineer's toolkit.

\bigskip
\begin{flushright}
\emph{ End of Manuscript }
\end{flushright}


\appendix
\part*{Appendices}
\addcontentsline{toc}{part}{Appendices}
\begin{center}
\emph{The details are not the details. They make the design.}\\
 Charles Eames
\end{center}
\bigskip

The appendices collect the technical material, proofs, and code snippets that support the main narrative. They are designed to be comprehensive, allowing the dedicated reader to reproduce every result presented in this monograph.

\bigskip
\hrule
\bigskip

\newpage

\section{Proofs and Derivations}
\label{app:proofs}

\subsection{Proof Sketch for the Ma-Yong Theorem \ref{thm:YongMa}}
\label{app:YongMaProof_appendix}
\begin{leanproofbox}{Proof Sketch: Ma-Yong Theorem \ref{thm:YongMa}}
The proof establishes existence and uniqueness for the fully coupled FBSDE \eqref{eq:FBSDE_canonical} under the monotonicity condition \ref{H3}.

\begin{enumerate}
    \item \textbf{Define the Mapping:} Construct a map \(\Phi\) on the space of solutions \(\mathcal{S}^2 \times \mathcal{H}^2\). Given a pair of processes \((\bar{Y}, \bar{Z})\), \(\Phi\) produces a new pair \((Y,Z)\) as follows:
    \begin{enumerate}
        \item Solve the forward SDE for \(X\) using \((\bar{Y}, \bar{Z})\) in the coefficients:
        \[ dX_t = \bm\mu(t, X_t, \bar{Y}_t, \bar{Z}_t)dt + \bm\sigma(t, X_t, \bar{Y}_t)dW_t. \]
        Under \ref{H1}-\ref{H2}, this SDE has a unique solution.
        \item With this path \(X\), solve the backward SDE for \((Y,Z)\):
        \[ dY_t = -f(t, X_t, Y_t, Z_t)dt + Z_t dW_t, \quad Y_T = g(X_T). \]
        This is a standard BSDE which has a unique solution under \ref{H1}-\ref{H2}.
    \end{enumerate}
    A solution to the original FBSDE is a fixed point of this map, i.e., \(\Phi(Y,Z) = (Y,Z)\).

    \item \textbf{Show Contraction:} The core of the proof is to show that \(\Phi\) is a contraction mapping on a suitable space. This is where the monotonicity condition \ref{H3} is essential. Consider two inputs \((\bar{Y}^1, \bar{Z}^1)\) and \((\bar{Y}^2, \bar{Z}^2)\) and their corresponding outputs \((Y^1, Z^1)\) and \((Y^2, Z^2)\). Let \(\delta Y = Y^1 - Y^2\), etc. Applying It么's formula to \(e^{\beta t}\|\delta Y_t\|^2\) for a well-chosen \(\beta > 0\) and using the monotonicity condition \ref{H3} leads to an inequality of the form:
    \[ \mathbb{E}[e^{\beta t}\|\delta Y_t\|^2] + \mathbb{E}\left[\int_t^T e^{\beta s}\|\delta Z_s\|^2 ds\right] \le C \mathbb{E}\left[\int_t^T e^{\beta s} (\|\delta \bar{Y}_s\|^2 + \|\delta \bar{Z}_s\|^2) ds\right]. \]
    For a sufficiently small time horizon \(T\), or by choosing \(\beta\) large enough, the map \(\Phi\) becomes a contraction on the weighted space.

    \item \textbf{Extend to Global Solution:} The contraction property on a small interval \([T-\delta, T]\) guarantees a unique solution on that interval. This solution can then be "stitched" together backward in time, extending the uniqueness to the full interval \([0,T]\).
\end{enumerate}
The Banach fixed-point theorem then ensures the existence of a unique solution.
\end{leanproofbox}

\subsection{Derivations for the Two-Agent GE Model}
\label{app:WealthShare}
\begin{leanproofbox}{Technical Derivations}
\paragraph{A. Derivation of the Wealth Share SDE (Eq. \ref{eq:wealth_share_SDE_new_main}).}
We derive the SDE for \(X_t = W_{1t}/S_t\). Applying It么's quotient rule:
\begin{equation*}
\frac{dX_t}{X_t} = \frac{dW_{1t}}{W_{1t}} - \frac{dS_t}{S_t} - \text{Cov}_t\left(\frac{dW_{1t}}{W_{1t}}, \frac{dS_t}{S_t}\right) + \text{Var}_t\left(\frac{dS_t}{S_t}\right).
\end{equation*}
The diffusion term is \(\sigma_X/X_t = \sigma_{W_1} - \sigma_S\). Using the optimal portfolios \(\pi_{it}^* = \kappa_t/(\gamma_i \sigma_{St})\), we have \(\sigma_{W_i} = \pi_{it}^*\sigma_{St} = \kappa_t/\gamma_i\). Market clearing implies \(\sigma_S = X_t \sigma_{W_1} + (1-X_t)\sigma_{W_2}\).
\begin{align*}
\sigma_X &= X_t(\sigma_{W_1} - \sigma_S) = X_t(1-X_t)(\sigma_{W_1}-\sigma_{W_2}) \\
&= X_t(1-X_t)\kappa_t\left(\frac{1}{\gamma_1}-\frac{1}{\gamma_2}\right).
\end{align*}
The drift term involves substituting the drifts of \(W_{1t}\) and \(S_t\), incorporating consumption rates \(c_{it}/W_{it} = 1/Y^i_t\), and the covariance terms. After algebraic simplification, it yields an expression for $\mu_X$.

\paragraph{B. Derivation of the Transformed SDE for \(\mathcal{X}_t\).}
We apply It么's lemma to \(\mathcal{X}_t = h(X_t) = \log(X_t/(1-X_t))\). The derivatives are \(h'(x) = \frac{1}{x(1-x)}\) and \(h''(x) = -\frac{1-2x}{(x(1-x))^2}\).
\begin{equation*}
d\mathcal{X}_t = h'(X_t) dX_t + \frac{1}{2} h''(X_t) (dX_t)^2.
\end{equation*}
The diffusion term is \(\sigma_{\mathcal{X}} = h'(X_t)\sigma_X(t) = \frac{1}{X_t(1-X_t)} \sigma_X(t)\), which simplifies to $\kappa_t(\frac{1}{\gamma_1}-\frac{1}{\gamma_2})$.
The drift term is \(\mu_{\mathcal{X}} = h'(X_t)\mu_X(t) + \frac{1}{2} h''(X_t)\sigma_X(t)^2$. Substituting the expressions for \(\mu_X\) and \(\sigma_X\) and simplifying yields the final drift for $\mathcal{X}_t$.
\end{leanproofbox}

\section{Benchmark Problem Specifications}
\label{app:benchmark_details}

This appendix provides the explicit coefficient functions for the benchmark problems introduced in Part \ref{part:benchmarks}.

\subsection{B.1: Allen-Cahn Equation}
The FBSDE system is \(dX_t = dW_t\) and \(dY_t = -f(Y_t)dt + Z_t dW_t\).
\begin{itemize}
    \item \textbf{Forward drift} \(\bm\mu(t,x,y,z) = 0\).
    \item \textbf{Forward volatility} \(\bm\sigma(t,x,y) = I_d\) (identity matrix).
    \item \textbf{BSDE driver} \(f(t,x,y,z) = y - y^3\).
    \item \textbf{Terminal condition} A common choice is \(g(x) = \frac{1}{1+\exp(x_1)}\) or another sigmoid-like function to initialize the phase separation.
\end{itemize}

\subsection{B.2: HJB Equation (LQ Control)}
Consider controlling \(dX_t = \alpha_t dt + \sqrt{2}dW_t\) to minimize \(\mathbb{E}[\int_0^T (\alpha_t^2 + \|X_t\|^2) dt + \|X_T\|^2]\). The value function \(u(t,x)\) solves \(\partial_t u + \Delta u - \frac{1}{2} \|\nabla u\|^2 + \frac{1}{2}\|x\|^2 = 0\).
\begin{itemize}
    \item \textbf{Forward SDE:} The optimal control is \(\alpha_t^* = -\nabla u(t,X_t)\). The PDE solution is \(u(t,x) = \coth(T-t+c)\|x\|^2/2 + \dots\), so \(\nabla u = \coth(T-t+c)x\). This gives a coupled system.
    \item \textbf{BSDE driver} \(f(t,x,y,z) = -\frac{1}{2}\|z\|^2 + \frac{1}{2}\|x\|^2\).
    \item \textbf{Terminal condition} \(g(x) = \|x\|^2\).
\end{itemize}

\subsection{B.3: Nonlinear Black-Scholes}
The FBSDE system is \(dX_t = rX_t dt + \sigma X_t dW_t\) and \(dY_t = -f(Y_t, Z_t)dt + Z_t dW_t\).
\begin{itemize}
    \item \textbf{Forward drift} \(\mu(t,x) = rx\).
    \item \textbf{Forward volatility} \(\sigma(t,x) = \sigma x\).
    \item \textbf{BSDE driver} \(f(t,x,y,z) = -ry + R(y - z/\sigma)\), where \(R\) is a nonlinear function, e.g., \(R(q) = \delta |q|\).
    \item \textbf{Terminal condition} \(g(x) = (x-K)^+\) for a European call option.
\end{itemize}

\section{Computational Appendix: The Two-Agent GE Model Implementation}
\label{app:GE_impl}
\addcontentsline{toc}{section}{Appendix C: GE Model Implementation}

This appendix provides a detailed, pedagogical walkthrough of the complete JAX implementation for solving the two-agent general equilibrium model. The goal is to guide a graduate student through each component, explaining not just the "what" but the "why" of the code structure and numerical choices. We implement the numerically stabilized system using the Logit/Log transformations (Section \ref{subsec:GE_transforms}) and the Deep BSDE method with a Heun integrator and a consistency-regularized loss, paying special attention to the QBSDE structure.

\subsection{C.1: Preamble and Reproducibility Audit}
Before defining the model, we perform a reproducibility audit. This ensures that any user running this code is aware of the exact software environment and configurations, which is critical for numerical work. We mandate 64-bit precision for accuracy, especially given the sensitivity of the QBSDE dynamics and the small correction terms in the Heun integrator.

\begin{lstlisting}[language=Python, caption={C.1: Reproducibility and Precision Audit.}, label={lst:audit_app_appendix}]
import jax
import jax.numpy as jnp
import jax.random as jr
from typing import Tuple
import equinox as eqx
import optax

# --- C.1.1: Mandate float64 for precision ---
# The Heun scheme's corrector step and QBSDE dynamics involve small differences,
# making 64-bit precision essential for numerical stability and accuracy.
jax.config.update("jax_enable_x64", True)

# --- C.1.2: Log environment for reproducibility ---
# JAX's default PRNG changed after version 0.4.25. We rely on the modern 'threefry'
# implementation and fixed seeds for reproducibility.
print("--- COMPUTATIONAL ENVIRONMENT AUDIT ---")
print(f"JAX version: {jax.__version__}")
print(f"X64 enabled: {jax.config.jax_enable_x64}")
print(f"Default PRNG implementation: {jax.config.jax_default_prng_impl}")
print("------------------------------------")
\end{lstlisting}

\subsection{C.2: Defining the Transformed Economic Model}
Our first step is to encapsulate all the economic logic into a single, well-defined class. We use an `equinox.Module`, which behaves like a Python dataclass but is compatible with JAX's functional programming paradigm (JIT compilation, automatic differentiation, etc.). This class will hold all model parameters and implement the functions for the transformed FBSDE system.

\begin{lstlisting}[language=Python, caption={C.2: Transformed Lucas Model FBSDE Coefficients in JAX.}, label={lst:lucas_jax_transformed_app_appendix}]
class LucasGEModel(eqx.Module):
    """
    Defines the primitives and transformed FBSDE system for the GE model.
    This module holds all parameters and implements the core economic functions.
    """
    # --- Economic Primitives ---
    # `eqx.static_field` tells Equinox that these fields are compile-time 
    # constants, not trainable parameters. This is crucial for efficient JIT compilation.
    mu_D: float = eqx.static_field()
    sigma_D: float = eqx.static_field()
    T: float = eqx.static_field()
    N: int = eqx.static_field()
    
    # Agent-specific parameters. These are fixed but not marked static, as one
    # might want to perform inference on them in other contexts.
    rho: jnp.ndarray  # Shape (2,) for two agents' discount rates
    gamma: jnp.ndarray # Shape (2,) for risk aversions

    # --- Derived Numerical Constants ---
    @property
    def h(self): 
        """Time step size."""
        return self.T / self.N
    @property
    def sqrt_h(self): 
        """Square root of time step size, for scaling Brownian increments."""
        return jnp.sqrt(self.h)

    def _get_equilibrium(self, cal_X, cal_Y):
        """
        Computes key equilibrium quantities from transformed state variables.
        cal_X: logit(X), cal_Y: log(Y)
        """
        # 1. Inverse transform state variables to their economic meaning
        X = jax.nn.sigmoid(cal_X) # logit^{-1} -> wealth share in (0,1)
        
        # 2. Compute consumption shares (lambda_c)
        # CRITICAL: Implemented using log-space arithmetic for numerical stability,
        # especially when X is close to 0 or 1. The naive formula:
        # lambda_c1 = (X/exp(cal_Y1)) / (X/exp(cal_Y1) + (1-X)/exp(cal_Y2))
        # is prone to underflow/overflow.
        log_c1_numerator = jnp.log(X) - cal_Y[..., 0]
        log_c2_numerator = jnp.log(1.0 - X) - cal_Y[..., 1]
        # jax.nn.logaddexp(a,b) is a stable way to compute log(exp(a)+exp(b))
        log_denominator = jnp.logaddexp(log_c1_numerator, log_c2_numerator)
        
        lambda_c1 = jnp.exp(log_c1_numerator - log_denominator)
        lambda_c2 = 1.0 - lambda_c1
        
        # 3. Compute aggregate risk aversion and prices, as per theory
        inv_Gamma = (lambda_c1 / self.gamma[0]) + (lambda_c2 / self.gamma[1])
        Gamma = 1.0 / inv_Gamma
        kappa = Gamma * self.sigma_D # Market price of risk
        R_agg = lambda_c1 * self.rho[0] + lambda_c2 * self.rho[1] # Agg. discount rate
        # Generalized Ramsey rule for the risk-free rate
        r = R_agg + Gamma * self.mu_D - 0.5 * Gamma * (Gamma + 1.0) * self.sigma_D**2
        return kappa, r, X, lambda_c1, lambda_c2

    def forward_sde(self, t, cal_X, cal_Y):
        """Computes drift (mu_calX) and volatility (sigma_calX) of the transformed forward process."""
        kappa, _, X, _, _ = self._get_equilibrium(cal_X, cal_Y)
        risk_tol_diff = (1.0 / self.gamma[0]) - (1.0 / self.gamma[1])
        
        # Volatility of transformed process cal_X is constant w.r.t. X
        sigma_calX = kappa * risk_tol_diff
        
        # Ito drift for cal_X (logit transform) requires an Ito correction term
        mu_X_untransformed = X * (1.0 - X) * (jnp.exp(-cal_Y[..., 1]) - jnp.exp(-cal_Y[..., 0]))
        ito_correction = -0.5 * (1.0 - 2.0 * X) * sigma_calX**2
        mu_calX = (mu_X_untransformed / (X * (1.0 - X))) + ito_correction
        return mu_calX, sigma_calX

    def driver(self, t, cal_X, cal_Y, cal_Z):
        """Computes the transformed driver F for the backward process cal_Y."""
        kappa, r, _, _, _ = self._get_equilibrium(cal_X, cal_Y)
        # Reshape for broadcasting against agent-specific dimensions
        rho_b = jnp.expand_dims(self.rho, 0)
        gamma_b = jnp.expand_dims(self.gamma, 0)
        r_b = jnp.expand_dims(r, -1)
        kappa_b = jnp.expand_dims(kappa, -1)

        # This is the driver for d(cal_Y) = -F dt + cal_Z dW, derived via Ito's lemma
        # on cal_Y = log(Y). It has the QBSDE structure.
        intertemporal_hedging = ((1.0 - gamma_b) / gamma_b) * (r_b + 0.5 * gamma_b * kappa_b**2)
        ito_term_from_log = 0.5 * cal_Z**2
        F = (1.0 - rho_b - intertemporal_hedging) - (1.0 - gamma_b) * cal_Z * kappa_b + ito_term_from_log
        return F
        
    def terminal_g(self, cal_X):
        """Terminal condition approximation: log(1/rho_i)."""
        # This is the stationary value for a representative agent, used as an
        # approximation for the infinite horizon problem.
        g = -jnp.log(self.rho)
        # Tile to match the batch dimension of cal_X
        return jnp.broadcast_to(g, (cal_X.shape[0], 2))
\end{lstlisting}

\subsection{C.3: Neural Network Architecture (Equinox)}
We define the neural networks that will approximate the unknown functions. `ControlApproximator` learns the function \(Z(t,X)\). In the previous draft, this included a `tanh` saturation for stability. However, following the advanced methodology in Section \ref{subsec:GE_discretization_full_R3}, we now rely on the Cole-Hopf transformation to handle the quadratic term analytically. This simplifies the network architecture, as explicit stabilization is no longer required.

\begin{lstlisting}[language=Python, caption={C.3: Equinox Modules for the Deep BSDE Solver (Post Cole-Hopf).}, label={lst:equinox_mlp_app_appendix}]
class ControlApproximator(eqx.Module):
    """
    Neural network approximating the control Z(t, X) -> R^2.
    After the Cole-Hopf transformation, the BSDE is no longer quadratic,
    so we do not need explicit stabilization like tanh saturation.
    """
    mlp: eqx.nn.MLP
    T_max: float = eqx.static_field()

    def __init__(self, key, T_max):
        self.T_max = T_max
        # Input: 2 features (normalized time, transformed state cal_X)
        # Output: 2 features (cal_Z for agent 1, cal_Z for agent 2)
        self.mlp = eqx.nn.MLP(in_size=2, out_size=2, width_size=128, depth=3, 
                              activation=jax.nn.silu, key=key)

    def __call__(self, t, cal_X):
        # Normalize time to [0,1] as a standard practice for NNs
        t_norm = t / self.T_max
        # Ensure t_norm is broadcast to the same shape as cal_X for stacking
        t_b = jnp.broadcast_to(t_norm, cal_X.shape)
        # Input to MLP should be of shape (batch_size, 2)
        mlp_in = jnp.stack([t_b, cal_X], axis=-1)
        return self.mlp(mlp_in)

class InitialValue(eqx.Module):
    """A simple container for the trainable parameter Y0."""
    calY0: jnp.ndarray
    def __init__(self, guess): self.calY0 = guess
    def __call__(self): return self.calY0
\end{lstlisting}

\subsection{C.4: The Simulation Loop and Loss Function}
This is the heart of the solver. The \texttt{compute\_loss} function performs the forward simulation of the FBSDE system for a batch of paths and calculates the total loss. It is designed to be JIT-compiled for maximum performance. The Heun scheme is correctly applied to both forward and backward processes to reduce bias, and the consistency loss is implemented with a \texttt{stop\_gradient} to prevent the optimization from becoming degenerate.

\begin{lstlisting}[language=Python, caption={C.4: JAX Implementation of the Consistency-Regularized Loss with Heun Scheme.}, label={lst:jax_loss_fn_app_appendix}]
def compute_loss(models: Tuple, batch_calX0: jnp.ndarray, econ_model: LucasGEModel, 
                 key: jr.PRNGKey, lambda_C: float):
    model_calY0, model_Z = models
    h, N, sqrt_h = econ_model.h, econ_model.N, econ_model.sqrt_h
    keys = jr.split(key, N)
    batch_size = batch_calX0.shape[0]
    
    # Vectorize functions to operate over the batch dimension.
    # `jax.vmap` is a key transformation for batching operations on a GPU/TPU.
    vmap_Z = jax.vmap(model_Z, in_axes=(None, 0)) # Map over cal_X, keep t fixed
    vmap_fwd = jax.vmap(econ_model.forward_sde, in_axes=(None, 0, 0))
    vmap_driver = jax.vmap(econ_model.driver, in_axes=(None, 0, 0, 0))

    # `jax.lax.scan` is a functional loop, essential for JIT-compiling simulations.
    # It iterates over a sequence (time steps), carrying a state (X_k, Y_k).
    def heun_step(carry, step_data):
        calX_k, calY_k = carry
        k, key_step = step_data
        t_k = k * h
        dW = jr.normal(key_step, (batch_size, 1)) * sqrt_h
        
        # --- Predictor values (evaluated at time k) ---
        calZ_k = vmap_Z(t_k, calX_k)
        mu_X_k, sigma_X_k = vmap_fwd(t_k, calX_k, calY_k)
        F_k = vmap_driver(t_k, calX_k, calY_k, calZ_k)
        
        # --- Predictor step for X and Y ---
        calX_pred = calX_k + mu_X_k * h + sigma_X_k * dW[:, 0]
        calY_pred = calY_k - F_k * h + calZ_k * dW
        
        # --- Corrector values (evaluated at time k+1 using predicted values) ---
        calZ_pred = vmap_Z(t_k + h, calX_pred)
        mu_X_pred, _ = vmap_fwd(t_k + h, calX_pred, calY_pred)
        F_pred = vmap_driver(t_k + h, calX_pred, calY_pred, calZ_pred)
        
        # --- Heun Update for X and Y ---
        calX_kp1 = calX_k + 0.5 * (mu_X_k + mu_X_pred) * h + sigma_X_k * dW[:, 0]
        calY_kp1 = calY_k - 0.5 * (F_k + F_pred) * h + 0.5 * (calZ_k + calZ_pred) * dW
        
        # --- Loss Calculation ---
        # Calculate the one-step residual for the consistency loss.
        # We use stop_gradient on the target to prevent the loss from being zero by construction.
        target_Y_kp1 = jax.lax.stop_gradient(calY_k - 0.5 * (F_k + F_pred) * h + 0.5 * (calZ_k + calZ_pred) * dW)
        residual = calY_kp1 - target_Y_kp1
        
        return (calX_kp1, calY_kp1), jnp.sum(residual**2, axis=-1)

    calY0_batch = jnp.broadcast_to(model_calY0(), (batch_size, 2))
    init_carry = (batch_calX0, calY0_batch)
    
    (calX_N, calY_N), residuals_sq_hist = jax.lax.scan(
        heun_step, init_carry, (jnp.arange(N), keys)
    )
    
    # --- Total Loss Computation ---
    # L_T: Terminal Loss (standard Deep BSDE loss)
    g_XN = econ_model.terminal_g(calX_N)
    terminal_loss = jnp.mean(jnp.sum((calY_N - g_XN)**2, axis=-1))
    
    # L_C: Consistency Loss (Level 0 of Malliavin-AD Tower)
    consistency_loss = jnp.mean(residuals_sq_hist)

    total_loss = terminal_loss + lambda_C * consistency_loss
    return total_loss, {"L_T": terminal_loss, "L_C": consistency_loss}

# Define the function that computes loss and gradients.
# This is done once at the top level for efficiency.
# `has_aux=True` tells JAX that our loss function returns auxiliary data (the dict).
loss_and_grad_fn = eqx.filter_value_and_grad(compute_loss, has_aux=True)
\end{lstlisting}

\subsection{C.5: Training Setup and Acceptance Tests}
The main training loop uses Optax for optimization and calls the JIT-compiled \texttt{make\_step}. After training, it runs a suite of acceptance tests to verify the economic and numerical consistency of the learned solution. These tests are crucial for building confidence in the results and correspond directly to the theoretical properties discussed in the main text.

\begin{lstlisting}[language=Python, caption={C.5: Training Loop, JIT-compiled Step, and Acceptance Tests.}, label={lst:jax_training_app_appendix}]
@eqx.filter_jit
def make_step(models, opt_state, key, econ_model, batch_size, lambda_C, optimizer):
    """Performs one step of optimization. JIT-compiled for efficiency."""
    # We start from a fixed initial state for simplicity (logit(0.5) = 0).
    batch_calX0 = jnp.zeros((batch_size,))
    
    # Compute loss and gradients by calling the pre-defined function.
    (loss, aux), grads = loss_and_grad_fn(models, batch_calX0, econ_model, key, lambda_C)
    
    # Update parameters using the optimizer.
    updates, opt_state = optimizer.update(grads, opt_state, models)
    models = eqx.apply_updates(models, updates)
    return models, opt_state, loss, aux

def run_acceptance_tests(trained_models, econ_model):
    """Runs a suite of tests to verify the solution."""
    print("\n--- RUNNING ACCEPTANCE TESTS ON FINAL MODEL ---")
    model_calY0, model_Z = trained_models
    
    # T1: Market Clearing Test
    print("T1: Verifying market clearing for consumption...")
    calY0 = model_calY0()
    test_calX = jnp.array([-10.0, -1.0, 0.0, 1.0, 10.0])
    vmap_eq = jax.vmap(econ_model._get_equilibrium, in_axes=(0, 0))
    calY_tiled = jnp.broadcast_to(calY0, (test_calX.shape[0], 2))
    _, _, _, lambda_c1, lambda_c2 = vmap_eq(test_calX, calY_tiled)
    total_share = lambda_c1 + lambda_c2
    assert jnp.allclose(total_share, 1.0, atol=1e-12), "T1 Failed: Market clearing!"
    print(" T1 Passed: Market Clearing holds.")

    # T2: Boundary Safety Test
    print("T2: Verifying boundary safety...")
    key_test = jr.PRNGKey(42)
    test_batch_size = 10000
    test_calX0 = jnp.zeros((test_batch_size,))
    init_carry = (test_calX0, jnp.broadcast_to(calY0, (test_batch_size, 2)))
    
    vmap_fwd_test = jax.vmap(econ_model.forward_sde, in_axes=(None, 0, 0))

    def test_sim_step(carry, step_data):
        calX_k, calY_k = carry
        t_k = step_data * econ_model.h
        dW = jr.normal(jr.PRNGKey(step_data), (test_batch_size, 1)) * econ_model.sqrt_h
        mu_X_k, sigma_X_k = vmap_fwd_test(t_k, calX_k, calY_k)
        calX_kp1 = calX_k + mu_X_k * econ_model.h + sigma_X_k * dW[:,0]
        return (calX_kp1, calY_k), calX_kp1

    _, X_hist = jax.lax.scan(test_sim_step, init_carry, jnp.arange(econ_model.N))
    
    X_untransformed = jax.nn.sigmoid(X_hist)
    assert jnp.all(X_untransformed > 1e-15) and jnp.all(X_untransformed < 1.0 - 1e-15), "T2 Failed: Boundary hit!"
    print(" T2 Passed: Boundaries are inaccessible.")
    print("---------------------------------------------")

# --- Main Execution Block ---
if __name__ == '__main__':
    key = jr.PRNGKey(20250811)
    key_model, key_train = jr.split(key)
    
    params = {'mu_D': 0.02, 'sigma_D': 0.2, 'rho': jnp.array([0.03, 0.04]), 
              'gamma': jnp.array([2.0, 5.0]), 'T': 30.0, 'N': 100}
    econ_model = LucasGEModel(**params)
    
    model_Z = ControlApproximator(key_model, econ_model.T)
    model_calY0 = InitialValue(-jnp.log(econ_model.rho))
    models = (model_calY0, model_Z)
    
    optimizer = optax.adamw(learning_rate=1e-3)
    opt_state = optimizer.init(eqx.filter(models, eqx.is_array))

    print("\nStarting JAX/Equinox training with Heun integrator and consistency loss...")
    for step in range(5001):
        key_train, key_batch = jr.split(key_train)
        models, opt_state, loss, aux = make_step(
            models, opt_state, key_batch, econ_model, 2048, lambda_C=0.5, optimizer=optimizer
        )
        if step % 1000 == 0:
            final_Y0_module, _ = models
            Y0_untransformed = jnp.exp(final_Y0_module.calY0)
            print(f"Step {step:4d} | Loss: {loss:.5f} | L_T: {aux['L_T']:.5f} | L_C: {aux['L_C']:.5f} "
                  f"| Y0_1: {Y0_untransformed[0]:.4f}, Y0_2: {Y0_untransformed[1]:.4f}")

    run_acceptance_tests(models, econ_model)
\end{lstlisting}

\newpage
\begin{filecontents*}{\jobname.bib}
@incollection{Bachouch2022,
  title={Deep learning for pricing and hedging of derivatives on assets with jumps},
  author={Bachouch, A. and Hur{\'e}, C. and Langren{\'e}, N. and Pham, H.},
  booktitle={Machine Learning and Data Sciences for Financial Markets},
  pages={209--246},
  year={2022},
  publisher={Chapman and Hall/CRC}
}
@article{BarlesBuckdahnPardoux1997,
  title={Backward stochastic differential equations and integral-partial differential equations},
  author={Barles, G. and Buckdahn, R. and Pardoux, E.},
  journal={Stochastics and Stochastics Reports},
  volume={60},
  number={1-2},
  pages={57--83},
  year={1997}
}
@article{Beck2019,
    title={Deep splitting method for parabolic {PDEs}},
    author={Beck, Christian and Becker, Sebastian and Cheridito, Patrick and Jentzen, Arnulf and Neufeld, Ariel},
    journal={arXiv preprint arXiv:1907.03452},
    year={2019}
}
@article{BenderSteiner2012,
  title={A Posteriori Estimates for Backward SDEs},
  author={Bender, C. and Steiner, J.},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={1},
  number={1},
  pages={139--163},
  year={2012}
}
@article{Carmona2016,
  title={Mean field games with common noise},
  author={Carmona, R. and Delarue, F. and Lacker, D.},
  journal={The Annals of Probability},
  volume={44},
  number={6},
  pages={3740--3803},
  year={2016}
}
@article{ChanKogan2002,
  title={Catching up with the Joneses: Heterogeneous preferences and the dynamics of asset prices},
  author={Chan, Y. L. and Kogan, L.},
  journal={The Journal of Finance},
  volume={57},
  number={6},
  pages={2689--2725},
  year={2002}
}
@article{CheriditoNam2017,
  title={Multidimensional quadratic and subquadratic BSDEs with special structure},
  author={Cheridito, Patrick and Nam, Kihun},
  journal={Stochastics},
  volume={89},
  number={1},
  pages={239--255},
  year={2017}
}
@article{ContFournie2013,
  title={Functional It{\^o} calculus and stochastic integral representation of martingales},
  author={Cont, R. and Fourni{\'e}, D. A.},
  journal={The Annals of Probability},
  volume={41},
  number={1},
  pages={109--133},
  year={2013}
}
@article{DomelevoWarin2023,
  title={The one-step Malliavin scheme for BSDEs},
  author={Domelevo, K. and Lemor, J. P. and Warin, X.},
  journal={arXiv preprint arXiv:2301.09194},
  year={2023}
}
@book{Duffie2001,
  title={Dynamic Asset Pricing Theory (3rd ed.)},
  author={Duffie, D.},
  year={2001},
  publisher={Princeton University Press}
}
@article{Dumas1989,
  title={Two-Person Dynamic Equilibrium in the Capital Market},
  author={Dumas, B.},
  journal={The Review of Financial Studies},
  volume={2},
  number={2},
  pages={157--188},
  year={1989}
}
@article{EHanJentzen2017,
  title={Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations},
  author={E, W. and Han, J. and Jentzen, A.},
  journal={Communications in Mathematics and Statistics},
  volume={5},
  number={4},
  pages={349--380},
  year={2017}
}
@article{FigueroaNisen2018,
  title={On the path behavior of the solution to the stochastic mortgage problem},
  author={Figueroa-L{\'o}pez, J. E. and Nisen, J.},
  journal={arXiv preprint arXiv:1804.06941},
  year={2018}
}
@article{ForetEtAl2021,
  title={Sharpness-Aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={International Conference on Learning Representations (ICLR)},
  year={2021}
}
@article{Giles2008,
  title={Multilevel Monte Carlo path simulation},
  author={Giles, M. B.},
  journal={Operations Research},
  volume={56},
  number={3},
  pages={607--617},
  year={2008}
}
@article{Gobet2005,
  title={A regression-based Monte Carlo method to solve backward stochastic differential equations},
  author={Gobet, E. and Lemor, J. P. and Warin, X.},
  journal={The Annals of Applied Probability},
  volume={15},
  number={3},
  pages={2172--2202},
  year={2005}
}
@article{Goudenegge2020,
  title={Convergence of a Robust Deep FBSDE Method for Stochastic Control},
  author={Gouden\`ege, L. and Molent, A. and Zanette, A.},
  journal={SIAM Journal on Numerical Analysis},
  volume={58},
  number={6},
  pages={3236--3261},
  year={2020}
}
@article{GuDao2024_Mamba,
  title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2024},
  note={Accepted at ICLR 2024}
}
@article{GMADDPG2023,
  title={Graph MADDPG with RNN for multiagent cooperative environment},
  author={Khan, M. A. and others},
  journal={Frontiers in Neurorobotics},
  volume={17},
  pages={1169535},
  year={2023}
}
@article{Hure2020,
    title={Deep backward dynamic programming for stochastic control problems},
    author={Hur{\'e}, Charles and Pham, Huy{\^e}n and Warin, Xavier},
    journal={Mathematics of Operations Research},
    volume={45},
    number={4},
    pages={1593--1628},
    year={2020}
}
@article{HuTang2016,
  title={Multi-dimensional backward stochastic differential equations of diagonally quadratic generators},
  author={Hu, Ying and Tang, Shanjian},
  journal={Stochastic Processes and their Applications},
  volume={126},
  number={4},
  pages={1066--1086},
  year={2016}
}
@book{KaratzasShreve1991,
  title={Brownian Motion and Stochastic Calculus (2nd ed.)},
  author={Karatzas, I. and Shreve, S. E.},
  year={1991},
  publisher={Springer}
}
@article{Kobylanski2000,
  title={Backward stochastic differential equations and partial differential equations with quadratic growth},
  author={Kobylanski, Magdalena},
  journal={The Annals of Probability},
  volume={28},
  number={2},
  pages={558--602},
  year={2000}
}
@article{MaYinZhang2008,
  title={On the numerical approximation of a class of infinite horizon BSDEs},
  author={Ma, J. and Yin, H. and Zhang, J.},
  journal={Stochastic Processes and their Applications},
  volume={118},
  number={6},
  pages={989--1005},
  year={2008}
}
@book{MaYong1999,
  title={Forward-backward stochastic differential equations and their applications},
  author={Ma, J. and Yong, J.},
  year={1999},
  publisher={Springer}
}
@book{Nualart2006,
  title={The Malliavin calculus and related topics (2nd ed.)},
  author={Nualart, D.},
  year={2006},
  publisher={Springer}
}
@article{PardouxPeng1990,
  title={Adapted solution of a backward stochastic differential equation},
  author={Pardoux, E. and Peng, S.},
  journal={Systems \& Control Letters},
  volume={14},
  number={1},
  pages={55--61},
  year={1990}
}
@incollection{PardouxPeng1992,
  title={Backward stochastic differential equations and quasilinear parabolic partial differential equations},
  author={Pardoux, E. and Peng, S.},
  booktitle={Stochastic partial differential equations and their applications},
  pages={200--217},
  year={1992},
  publisher={Springer}
}
@article{ParkTu2025,
  title={Integration Matters for Learning PDEs with Backwards SDEs},
  author={Park, S. and Tu, S.},
  journal={arXiv preprint arXiv:2505.01078},
  year={2025}
}
@inproceedings{MAGNet2019,
  title={MAGNet: Multi-agent graph network for deep multi-agent reinforcement learning},
  author={Qureshi, A. H. and others},
  booktitle={Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems},
  year={2019}
}
@inproceedings{Tancik2020,
  title={Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
  author={Tancik, Matthew and others},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  year={2020}
}
@article{Tevzadze2008,
  title={Solvability of backward stochastic differential equations with quadratic growth},
  author={Tevzadze, Revaz},
  journal={Stochastic Processes and their Applications},
  volume={118},
  number={3},
  pages={503--515},
  year={2008}
}
@article{Warin2024,
  title={A posteriori error estimates for deep learning solutions of {FBSDE}s},
  author={Warin, X.},
  journal={SIAM Journal on Scientific Computing},
  volume={46},
  number={3},
  pages={A1540--A1565},
  year={2024}
}
\end{filecontents*}
\printbibliography

\end{document} 
