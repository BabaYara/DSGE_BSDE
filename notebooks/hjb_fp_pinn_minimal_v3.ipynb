{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b748c49b",
   "metadata": {},
   "source": [
    "\n",
    "# Minimal Stationary HJB–FP × PINN Solver (JAX + Equinox)\n",
    "\n",
    "**Scope:** Two PINNs on a rectangular box for a stationary HJB–Fokker–Planck fixed point with partial irreversibility (smooth buy/sell wedge), plus a damped price-clearing loop.  \n",
    "**Dependencies:** `jax`, `equinox`, `optax`, `sympy`, `matplotlib`.\n",
    "\n",
    "**Model:**\n",
    "- States: capital \\(k\\in[k_{\\min},k_{\\max}]\\), productivity shock \\(z\\in[-z_{\\max}, z_{\\max}]\\) with OU dynamics \\(\\mathrm{d}z = -\\kappa_z z\\,\\mathrm{d}t + \\sigma_z \\mathrm{d}B_t\\).\n",
    "- Output: \\(y(k,z;x)=\\exp(x+z)k^\\alpha\\). Aggregate \\(Y=\\iint y\\,\\mu\\,\\mathrm{d}k\\,\\mathrm{d}z\\). Inverse demand \\(P=Y^{-\\eta}\\).\n",
    "- Capital law: \\(\\dot k = i - \\delta k\\).\n",
    "- Irreversibility cost (smoothed): instantaneous Hamiltonian term \\(\\sup_i \\{ (V_k-1)i - \\frac{\\theta(i)}{2}\\frac{i^2}{k} \\}\\) with\n",
    "  a differentiable switch \\(\\theta(V_k) \\in \\{\\theta_+,\\theta_-\\}\\) around \\(V_k=1\\).\n",
    "- HJB residual:\n",
    "\\[\n",
    "\\mathcal{R}_{\\rm HJB} = rV - \\Big[ P e^{x+z}k^\\alpha - \\delta k\\,V_k + \\frac{k}{2\\,\\theta(V_k)}(V_k-1)^2 + \\frac{1}{2}\\sigma_z^2 V_{zz} - \\kappa_z z\\,V_z \\Big].\n",
    "\\]\n",
    "- Stationary FP residual (no diffusion in \\(k\\)):\n",
    "\\[\n",
    "\\mathcal{R}_{\\rm FP} = -\\partial_k\\big((i^\\*-\\delta k)\\mu\\big) - \\partial_z\\big((-\\,\\kappa_z z)\\mu\\big) + \\tfrac12\\sigma_z^2\\,\\partial_{zz}\\mu.\n",
    "\\]\n",
    "- Control and fluxes:\n",
    "\\[\n",
    "i^\\*=\\frac{k}{\\theta(V_k)}(V_k-1),\\quad\n",
    "j_k=(i^\\*-\\delta k)\\mu,\\quad\n",
    "j_z=(-\\kappa_z z)\\mu - \\tfrac12\\sigma_z^2\\,\\partial_z \\mu.\n",
    "\\]\n",
    "\n",
    "**Losses:**\n",
    "- Interior: MSE of \\(\\mathcal{R}_{\\rm HJB}\\) and \\(\\mathcal{R}_{\\rm FP}\\).\n",
    "- Boundary: MSE of \\(j_k\\) on \\(k\\)-edges and \\(j_z\\) on \\(z\\)-edges.\n",
    "- Mass: \\( (\\widehat{\\int\\!\\!\\int\\mu}-1)^2\\) by Monte Carlo.\n",
    "- Tiny L2 on parameters.\n",
    "\n",
    "**Training loop:** Alternating training for \\(V\\) and \\(\\mu\\) inside an outer price iteration: \\(\\log P \\leftarrow (1-\\lambda)\\log P + \\lambda \\log(Y^{-\\eta})\\).\n",
    "\n",
    "> This is a small baseline. It is intentionally minimal and uses random collocation with autodiff (DGM/PINN-style). Scale `n_*` and `steps_*` to tighten residuals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "66abaae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) If running in a fresh environment, uncomment to install.\n",
    "# !pip install -q \"jax[cpu]\" equinox optax sympy matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38ca465c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, vmap, jit\n",
    "import optax\n",
    "import equinox as eqx\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# Enable float64 for second derivatives stability\n",
    "from jax import config as jax_config\n",
    "jax_config.update(\"jax_enable_x64\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "49a27f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Economics\n",
    "    r: float = 0.05\n",
    "    delta: float = 0.08\n",
    "    alpha: float = 0.35\n",
    "    eta: float = 1.5\n",
    "    xbar: float = 0.0\n",
    "    kappa_z: float = 0.5\n",
    "    sigma_z: float = 0.5\n",
    "    theta_plus: float = 0.5   # buy cost (lower)\n",
    "    theta_minus: float = 1.5  # sell cost (higher)\n",
    "    beta_switch: float = 20.0 # sharpness around Vk=1\n",
    "\n",
    "    # Box domain\n",
    "    k_min: float = 0.2\n",
    "    k_max: float = 5.0\n",
    "    z_max: float = 3.0\n",
    "\n",
    "    # Training sizes\n",
    "    n_colloc: int = 2048\n",
    "    n_bndry: int  = 1024\n",
    "    n_mass: int   = 4096\n",
    "    n_agg: int    = 4096\n",
    "\n",
    "    # Optimisation\n",
    "    steps_V: int = 600\n",
    "    steps_M: int = 600\n",
    "    outer_loops: int = 5\n",
    "    lr_V: float = 2e-3\n",
    "    lr_M: float = 2e-3\n",
    "    l2: float = 1e-6\n",
    "    w_bndry: float = 5.0\n",
    "    w_mass: float = 10.0\n",
    "\n",
    "    # Price iteration\n",
    "    P0: float = 1.0\n",
    "    lambda_price: float = 0.7\n",
    "\n",
    "cfg = Config()\n",
    "key = jax.random.PRNGKey(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73897353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Normalise inputs to [-1,1]^2 for network stability\n",
    "def norm_inputs(k, z, cfg: Config):\n",
    "    k_n = 2.0*(k - cfg.k_min)/(cfg.k_max - cfg.k_min) - 1.0\n",
    "    z_n = z/cfg.z_max\n",
    "    return jnp.stack([k_n, z_n], axis=-1)\n",
    "\n",
    "def unnorm_k(kn, cfg: Config):\n",
    "    return ( (kn + 1.0)*0.5 )*(cfg.k_max - cfg.k_min) + cfg.k_min\n",
    "\n",
    "# Sampling\n",
    "def sample_interior(key, n, cfg: Config):\n",
    "    k = jax.random.uniform(key, (n,), minval=cfg.k_min, maxval=cfg.k_max)\n",
    "    z = jax.random.uniform(jax.random.split(key, 2)[1], (n,), minval=-cfg.z_max, maxval=cfg.z_max)\n",
    "    return k, z\n",
    "\n",
    "def sample_edge_k(key, n, k_edge, cfg: Config):\n",
    "    z = jax.random.uniform(key, (n,), minval=-cfg.z_max, maxval=cfg.z_max)\n",
    "    k = jnp.full((n,), k_edge)\n",
    "    return k, z\n",
    "\n",
    "def sample_edge_z(key, n, z_edge, cfg: Config):\n",
    "    k = jax.random.uniform(key, (n,), minval=cfg.k_min, maxval=cfg.k_max)\n",
    "    z = jnp.full((n,), z_edge)\n",
    "    return k, z\n",
    "\n",
    "def area(cfg: Config):\n",
    "    return (cfg.k_max - cfg.k_min) * (2.0*cfg.z_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f41df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two small MLPs\n",
    "def make_mlp(key, width=64, depth=3):\n",
    "    # Equinox MLP signature: MLP(in_size, out_size, width_size, depth, *, activation, key)\n",
    "    return eqx.nn.MLP(in_size=2, out_size=1, width_size=width, depth=depth, activation=jax.nn.tanh, key=key)\n",
    "\n",
    "key, k1, k2 = jax.random.split(key, 3)\n",
    "V_net = make_mlp(k1, width=64, depth=3)   # value\n",
    "S_net = make_mlp(k2, width=64, depth=3)   # density pre-activation; mu = softplus(S)^2\n",
    "\n",
    "def mu_from_S(S):\n",
    "    return jax.nn.softplus(S)**2 + 1e-12  # positivity and avoid exact zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9f907a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Smooth switch for theta(V_k): small theta when Vk>1 (buy), large theta when Vk<1 (sell)\n",
    "def theta_of_Vk(Vk, cfg: Config):\n",
    "    # sigmoid(s) ~ 0 when Vk >> 1; ~1 when Vk << 1\n",
    "    s = -cfg.beta_switch*(Vk - 1.0)\n",
    "    sig = jax.nn.sigmoid(s)\n",
    "    return cfg.theta_plus + (cfg.theta_minus - cfg.theta_plus)*sig\n",
    "\n",
    "def i_star(k, Vk, cfg: Config):\n",
    "    theta = theta_of_Vk(Vk, cfg)\n",
    "    return (k/theta) * (Vk - 1.0)\n",
    "\n",
    "def price_from_Y(Y, cfg: Config):\n",
    "    return Y ** (-cfg.eta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5e9591f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Autodiff helpers\n",
    "def V_and_derivs(params_V, k, z, cfg: Config):\n",
    "    x = norm_inputs(k, z, cfg)\n",
    "    V = eqx.filter_eval_shape(lambda m, x: m(x).squeeze(), params_V, x)  # shape hint\n",
    "    V = V_net(x).squeeze()\n",
    "    # First derivatives\n",
    "    dV_dk = grad(lambda kk: V_net(norm_inputs(kk, z, cfg)).squeeze())(k)\n",
    "    dV_dz = grad(lambda zz: V_net(norm_inputs(k, zz, cfg)).squeeze())(z)\n",
    "    # Second derivative w.r.t z\n",
    "    d2V_dzz = grad(lambda zz: grad(lambda t: V_net(norm_inputs(k, t, cfg)).squeeze())(zz))(z)\n",
    "    return V, dV_dk, dV_dz, d2V_dzz\n",
    "\n",
    "def S_mu_and_derivs(params_S, k, z, cfg: Config):\n",
    "    x = norm_inputs(k, z, cfg)\n",
    "    S = S_net(x).squeeze()\n",
    "    mu = mu_from_S(S)\n",
    "    # z-derivatives for diffusion flux and FP term\n",
    "    dmu_dz = grad(lambda zz: mu_from_S(S_net(norm_inputs(k, zz, cfg)).squeeze()))(z)\n",
    "    d2mu_dzz = grad(lambda zz: grad(lambda t: mu_from_S(S_net(norm_inputs(k, t, cfg)).squeeze()))(zz))(z)\n",
    "    return S, mu, dmu_dz, d2mu_dzz\n",
    "\n",
    "def hjb_residual(k, z, P, cfg: Config):\n",
    "    V, Vk, Vz, Vzz = V_and_derivs(V_net, k, z, cfg)\n",
    "    flow = P * jnp.exp(cfg.xbar + z) * (k**cfg.alpha)\n",
    "    term_inv = (k/(2.0*theta_of_Vk(Vk, cfg))) * (Vk - 1.0)**2\n",
    "    rhs = flow - cfg.delta * k * Vk + 0.5 * (cfg.sigma_z**2) * Vzz - cfg.kappa_z * z * Vz + term_inv\n",
    "    return cfg.r*V - rhs\n",
    "\n",
    "def fp_residual(k, z, P, cfg: Config):\n",
    "    _, mu, dmu_dz, d2mu_dzz = S_mu_and_derivs(S_net, k, z, cfg)\n",
    "    V, Vk, _, _ = V_and_derivs(V_net, k, z, cfg)\n",
    "    drift_k = i_star(k, Vk, cfg) - cfg.delta * k\n",
    "    drift_z = -cfg.kappa_z * z\n",
    "    # Partial derivatives of drift*mu terms\n",
    "    def gk_mu(kk):\n",
    "        Vloc, Vkloc, _, _ = V_and_derivs(V_net, kk, z, cfg)\n",
    "        return (i_star(kk, Vkloc, cfg) - cfg.delta*kk) * mu_from_S(S_net(norm_inputs(kk, z, cfg)).squeeze())\n",
    "    d_gkmu_dk = grad(gk_mu)(k)\n",
    "    def gz_mu(zz):\n",
    "        return (-cfg.kappa_z * zz) * mu_from_S(S_net(norm_inputs(k, zz, cfg)).squeeze())\n",
    "    d_gzmu_dz = grad(gz_mu)(z)\n",
    "    return - d_gkmu_dk - d_gzmu_dz + 0.5*(cfg.sigma_z**2) * d2mu_dzz\n",
    "\n",
    "# Boundary fluxes\n",
    "def flux_k(k_edge, z, cfg: Config):\n",
    "    _, mu, _, _ = S_mu_and_derivs(S_net, k_edge, z, cfg)\n",
    "    _, Vk, _, _ = V_and_derivs(V_net, k_edge, z, cfg)\n",
    "    return (i_star(k_edge, Vk, cfg) - cfg.delta*k_edge) * mu\n",
    "\n",
    "def flux_z(k, z_edge, cfg: Config):\n",
    "    _, mu, dmu_dz, _ = S_mu_and_derivs(S_net, k, z_edge, cfg)\n",
    "    return (-cfg.kappa_z * z_edge) * mu - 0.5*(cfg.sigma_z**2) * dmu_dz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c37ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hjb_loss(batch_k, batch_z, P, cfg: Config):\n",
    "    res = vmap(lambda kk, zz: hjb_residual(kk, zz, P, cfg))(batch_k, batch_z)\n",
    "    return jnp.mean(res**2)\n",
    "\n",
    "def fp_loss(batch_k, batch_z, P, cfg: Config):\n",
    "    res = vmap(lambda kk, zz: fp_residual(kk, zz, P, cfg))(batch_k, batch_z)\n",
    "    return jnp.mean(res**2)\n",
    "\n",
    "def boundary_loss(kmin_samples, kmax_samples, zmin_samples, zmax_samples, cfg: Config):\n",
    "    jk_min = vmap(lambda zz: flux_k(cfg.k_min, zz, cfg))(kmin_samples[1])\n",
    "    jk_max = vmap(lambda zz: flux_k(cfg.k_max, zz, cfg))(kmax_samples[1])\n",
    "    jz_min = vmap(lambda kk: flux_z(kk, -cfg.z_max, cfg))(zmin_samples[0])\n",
    "    jz_max = vmap(lambda kk: flux_z(kk,  cfg.z_max, cfg))(zmax_samples[0])\n",
    "    return jnp.mean(jk_min**2) + jnp.mean(jk_max**2) + jnp.mean(jz_min**2) + jnp.mean(jz_max**2)\n",
    "\n",
    "def mass_estimate(key, cfg: Config):\n",
    "    k, z = sample_interior(key, cfg.n_mass, cfg)\n",
    "    mu_vals = vmap(lambda kk, zz: mu_from_S(S_net(norm_inputs(kk, zz, cfg)).squeeze()))(k, z)\n",
    "    return area(cfg) * jnp.mean(mu_vals)\n",
    "\n",
    "def aggregate_Y(key, P, cfg: Config):\n",
    "    k, z = sample_interior(key, cfg.n_agg, cfg)\n",
    "    mu_vals = vmap(lambda kk, zz: mu_from_S(S_net(norm_inputs(kk, zz, cfg)).squeeze()))(k, z)\n",
    "    y = jnp.exp(cfg.xbar + z) * (k**cfg.alpha)\n",
    "    return area(cfg) * jnp.mean(y * mu_vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "870eb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def loss_V(P, key, cfg: Config):\n",
    "    k, z = sample_interior(key, cfg.n_colloc, cfg)\n",
    "    l = hjb_loss(k, z, P, cfg)\n",
    "    # L2 regularization\n",
    "    l2 = sum([jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(eqx.filter(V_net, eqx.is_inexact_array))])\n",
    "    return l + cfg.l2 * l2\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def loss_M(P, keys, cfg: Config):\n",
    "    k, z = sample_interior(keys[0], cfg.n_colloc, cfg)\n",
    "    l_fp = fp_loss(k, z, P, cfg)\n",
    "    # boundary samples\n",
    "    kz1 = sample_edge_k(keys[1], cfg.n_bndry, cfg.k_min, cfg)\n",
    "    kz2 = sample_edge_k(keys[2], cfg.n_bndry, cfg.k_max, cfg)\n",
    "    kz3 = sample_edge_z(keys[3], cfg.n_bndry, -cfg.z_max, cfg)\n",
    "    kz4 = sample_edge_z(keys[4], cfg.n_bndry,  cfg.z_max, cfg)\n",
    "    l_b = boundary_loss(kz1, kz2, kz3, kz4, cfg)\n",
    "    # mass penalty\n",
    "    m = mass_estimate(keys[5], cfg)\n",
    "    l_mass = (m - 1.0)**2\n",
    "    # L2\n",
    "    l2 = sum([jnp.sum(jnp.square(p)) for p in jax.tree_util.tree_leaves(eqx.filter(S_net, eqx.is_inexact_array))])\n",
    "    return l_fp + cfg.w_bndry*l_b + cfg.w_mass*l_mass + cfg.l2*l2\n",
    "\n",
    "opt_V = optax.adam(cfg.lr_V)\n",
    "opt_M = optax.adam(cfg.lr_M)\n",
    "opt_state_V = opt_V.init(eqx.filter(V_net, eqx.is_inexact_array))\n",
    "opt_state_M = opt_M.init(eqx.filter(S_net, eqx.is_inexact_array))\n",
    "\n",
    "def train_outer(P, key, cfg: Config):\n",
    "    global V_net, S_net, opt_state_V, opt_state_M\n",
    "    # --- Train V ---\n",
    "    for t in range(cfg.steps_V):\n",
    "        key, sub = jax.random.split(key)\n",
    "        (lV), grads = loss_V(P, sub, cfg)\n",
    "        updates, opt_state_V = opt_V.update(grads, opt_state_V, params=eqx.filter(V_net, eqx.is_inexact_array))\n",
    "        V_net = eqx.apply_updates(V_net, updates)\n",
    "        if (t+1) % max(1, cfg.steps_V//3) == 0:\n",
    "            print(f\"  V step {t+1}/{cfg.steps_V}: L_HJB={float(lV):.3e}\")\n",
    "    # --- Train M ---\n",
    "    for t in range(cfg.steps_M):\n",
    "        key, *subs = jax.random.split(key, 7)\n",
    "        (lM), grads = loss_M(P, subs, cfg)\n",
    "        updates, opt_state_M = opt_M.update(grads, opt_state_M, params=eqx.filter(S_net, eqx.is_inexact_array))\n",
    "        S_net = eqx.apply_updates(S_net, updates)\n",
    "        if (t+1) % max(1, cfg.steps_M//3) == 0:\n",
    "            print(f\"  M step {t+1}/{cfg.steps_M}: L_FP+pen={float(lM):.3e}\")\n",
    "    # --- Diagnostics & price update ---\n",
    "    key, km, ka = jax.random.split(key, 3)\n",
    "    mhat = float(mass_estimate(km, cfg))\n",
    "    Yhat = float(aggregate_Y(ka, P, cfg))\n",
    "    P_new = math.exp((1-cfg.lambda_price)*math.log(P) + cfg.lambda_price*math.log(max(1e-12, Yhat**(-cfg.eta))))\n",
    "    return P_new, mhat, Yhat, key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5573799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Outer iteration 1/5, current P=1.0000 ===\n",
      "  V step 200/600: L_HJB=7.284e+01\n",
      "  V step 400/600: L_HJB=7.733e+01\n",
      "  V step 600/600: L_HJB=6.781e+01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run a short outer loop as a smoke test (increase steps_* and n_* for accuracy)\n",
    "P = cfg.P0\n",
    "key = jax.random.PRNGKey(42)\n",
    "for it in range(cfg.outer_loops):\n",
    "    print(f\"\\n=== Outer iteration {it+1}/{cfg.outer_loops}, current P={P:.4f} ===\")\n",
    "    P, mhat, Yhat, key = train_outer(P, key, cfg)\n",
    "    print(f\"Mass ~ {mhat:.4f}, Y ~ {Yhat:.4f}, updated P={P:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacaf5b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Quick visual: mu(k, z=0) profile after training (coarse grid)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01monp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m kgrid = onp.linspace(\u001b[43mcfg\u001b[49m.k_min, cfg.k_max, \u001b[32m200\u001b[39m)\n\u001b[32m      4\u001b[39m z0 = \u001b[32m0.0\u001b[39m\n\u001b[32m      5\u001b[39m mu_line = []\n",
      "\u001b[31mNameError\u001b[39m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Quick visual: mu(k, z=0) profile after training (coarse grid)\n",
    "import numpy as onp\n",
    "kgrid = onp.linspace(cfg.k_min, cfg.k_max, 200)\n",
    "z0 = 0.0\n",
    "mu_line = []\n",
    "for kk in kgrid:\n",
    "    mu_line.append(mu_from_S(S_net(norm_inputs(jnp.array(kk), jnp.array(z0), cfg)).squeeze()))\n",
    "mu_line = jnp.array(mu_line)\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(kgrid, onp.asarray(mu_line))\n",
    "plt.title(\"Stationary density slice: $\\\\mu(k, z=0)$\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"mu\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8865f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Verify i* and conjugate term with SymPy for constant theta\n",
    "import sympy as sp\n",
    "i, Vk, k, theta = sp.symbols('i Vk k theta', positive=True, real=True)\n",
    "expr = (Vk - 1)*i - (theta/2)*i**2/k\n",
    "istar = sp.simplify(sp.diff(expr, i))\n",
    "istar_sol = sp.solve(sp.Eq(istar, 0), i)[0]\n",
    "sup_val = sp.simplify(expr.subs(i, istar_sol))\n",
    "print(\"i* =\", istar_sol)\n",
    "print(\"Sup value =\", sup_val)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
