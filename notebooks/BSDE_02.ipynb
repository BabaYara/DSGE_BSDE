{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "399049b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main] shape-robust V_eval/grad_V/hess_V_xz active\n"
     ]
    }
   ],
   "source": [
    "# CRRA–Zhang (2005 JF) production economy via a compact FBSDE/HJB-residual scheme.\n",
    "# Features: q-rule controls (costly reversibility), DeepSets distribution-state,\n",
    "# tower (regress-later) penalty, KT forward/backward differential penalties,\n",
    "# Malliavin (BEL) regularizer, Tamed–Euler for K, gradient clipping + AdamW,\n",
    "# Polyak–Ruppert / SWA parameter averaging, and plots including D/P and NEW: log price p_t.\n",
    "#\n",
    "# Sources validated via web search through Sep 2025:\n",
    "# - q-kink & costly reversibility: Abel–Eberly (1996), Zhang (2005 JF)\n",
    "# - tower: Gobet–Lemor–Warin (2005); deep backward schemes (Huré–Pham–Warin 2019)\n",
    "# - KT forward/backward differential deep BSDE: Kapllani–Teng (2024a,b)\n",
    "# - Malliavin/BEL estimator: Fournié–Lasry–Lebuchoux–Lions–Touzi (1999)\n",
    "# - DeepSets for permutation-invariant distribution state: Zaheer et al. (2017); Wagstaff et al. (2022)\n",
    "# - Mean-field/state-as-distribution: Carmona–Delarue (2018)\n",
    "# - Tamed–Euler: Hutzenthaler–Jentzen–Kloeden (2012)\n",
    "# - Robustness: gradient clipping (Pascanu et al. 2013), AdamW (Loshchilov–Hutter 2017),\n",
    "#   Polyak–Ruppert averaging (1992) / SWA (Izmailov et al. 2018)\n",
    "#\n",
    "# Deps: jax, equinox, optax, numpy, matplotlib, sympy\n",
    "# (If sympy not found, symbolic check is skipped.)\n",
    "\n",
    "import math, functools\n",
    "import numpy as np\n",
    "import jax, jax.numpy as jnp\n",
    "import jax.tree_util as jtu\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (5.2, 3.2)\n",
    "\n",
    "try:\n",
    "    import sympy as sp\n",
    "    HAVE_SYMPY = True\n",
    "except Exception:\n",
    "    HAVE_SYMPY = False\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "class Cfg(eqx.Module):\n",
    "    # Technology\n",
    "    alpha: float = 0.70      # returns to capital (reduced form)\n",
    "    eta: float = 0.20        # inverse demand elasticity (price level responds to output)\n",
    "    f: float = 0.02          # fixed operating flow cost\n",
    "    delta: float = 0.08\n",
    "\n",
    "    # Shocks\n",
    "    kappa_x: float = 0.30    # OU mean reversion for aggregate TFP\n",
    "    mu_x: float = 0.0\n",
    "    sigma_x: float = 0.15\n",
    "    kappa_z: float = 1.00\n",
    "    sigma_z: float = 0.25\n",
    "\n",
    "    # Preferences (CRRA)\n",
    "    gamma: float = 5.0\n",
    "    rho: float = 0.02\n",
    "\n",
    "    # Adjustment costs (costly reversibility)\n",
    "    theta_plus: float = 4.0  # expansion\n",
    "    theta_minus: float = 10.0  # contraction\n",
    "\n",
    "    # Simulation\n",
    "    T: float = 3.0\n",
    "    nT: int = 128\n",
    "    nF: int = 1024\n",
    "    seed: int = 123\n",
    "    antithetic: bool = True\n",
    "\n",
    "    # Distribution (DeepSets) embedding size\n",
    "    m_dim: int = 4\n",
    "\n",
    "    # Training\n",
    "    steps: int = 400\n",
    "    lr: float = 1e-3\n",
    "\n",
    "    # Loss weights (self-evolving rubric will tweak these)\n",
    "    w_tower: float = 1.0\n",
    "    w_forward: float = 0.05\n",
    "    w_back: float = 1e-3\n",
    "    w_bel: float = 0.05\n",
    "\n",
    "cfg = Cfg()\n",
    "\n",
    "def clamp(x, lo=1e-9, hi=1e9):\n",
    "    return jnp.clip(x, lo, hi)\n",
    "\n",
    "# ------------- DeepSets for distribution state m_t -------------\n",
    "class DeepSet(eqx.Module):\n",
    "    phi: eqx.Module\n",
    "    rho: eqx.Module\n",
    "    m_dim: int\n",
    "    def __init__(self, key, m_dim=4, width=16):\n",
    "        k1,k2 = jax.random.split(key, 2)\n",
    "        self.phi = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(2, width, key=k1), eqx.nn.Lambda(jax.nn.tanh),\n",
    "            eqx.nn.Linear(width, width, key=k1), eqx.nn.Lambda(jax.nn.tanh)\n",
    "        ])\n",
    "        self.rho = eqx.nn.Sequential([\n",
    "            eqx.nn.Linear(width, 2*width, key=k2), eqx.nn.Lambda(jax.nn.tanh),\n",
    "            eqx.nn.Linear(2*width, m_dim, key=k2)\n",
    "        ])\n",
    "        self.m_dim = m_dim\n",
    "    def __call__(self, K, z):\n",
    "        X = jnp.stack([K, z], axis=-1)\n",
    "        h = jax.vmap(self.phi)(X)\n",
    "        H = jnp.sum(h, axis=0)  # permutation-invariant sum\n",
    "        return self.rho(H)\n",
    "\n",
    "# ------------- Value net V(K,z,x,m) -------------\n",
    "class VNet(eqx.Module):\n",
    "    net: eqx.Module\n",
    "    in_dim: int\n",
    "    def __init__(self, key, in_dim, width=64, depth=3):\n",
    "        layers = []\n",
    "        keys = jax.random.split(key, depth+1)\n",
    "        d = in_dim\n",
    "        for i in range(depth):\n",
    "            layers += [eqx.nn.Linear(d, width, key=keys[i]), eqx.nn.Lambda(jax.nn.tanh)]\n",
    "            d = width\n",
    "        layers += [eqx.nn.Linear(d, 1, key=keys[-1])]\n",
    "        self.net = eqx.nn.Sequential(layers)\n",
    "        self.in_dim = in_dim\n",
    "    def __call__(self, x):\n",
    "        K = x[...,0]\n",
    "        # Barrier near K≈0 so q-rule does not explode\n",
    "        barrier = jax.nn.sigmoid((K-0.05)/0.02)\n",
    "        y = self.net(x)\n",
    "        y = (barrier * y)[...,0]\n",
    "        return y\n",
    "\n",
    "# Shape helper to accept scalar or vector K,z,x\n",
    "_def_msg = \"[main] shape-robust V_eval/grad_V/hess_V_xz active\"\n",
    "print(_def_msg)\n",
    "\n",
    "def _batch3(K, z, x):\n",
    "    K = jnp.asarray(K)\n",
    "    z = jnp.asarray(z)\n",
    "    x = jnp.asarray(x)\n",
    "    if K.ndim == 0:\n",
    "        K = K[None]\n",
    "    if z.ndim == 0:\n",
    "        z = z[None]\n",
    "    # Broadcast x to match K's leading size\n",
    "    if x.shape != K.shape:\n",
    "        x = jnp.broadcast_to(x, K.shape)\n",
    "    return K, z, x\n",
    "\n",
    "def V_eval(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    m = jnp.asarray(m).reshape(-1)\n",
    "    n = K.shape[0]\n",
    "    M = jnp.broadcast_to(m, (n, m.shape[0]))\n",
    "    XYZ = jnp.stack([K, z, x], axis=-1)  # (n, 3)\n",
    "    X = jnp.concatenate([XYZ, M], axis=1)\n",
    "    # Apply model per-sample (Equinox layers are single-sample by default)\n",
    "    return jax.vmap(modelV)(X)\n",
    "\n",
    "def grad_V(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    U = jnp.stack([K, z, x], axis=-1)\n",
    "    m_vec = jnp.asarray(m).reshape(-1)\n",
    "    def f(u):\n",
    "        return modelV(jnp.concatenate([u, m_vec]))\n",
    "    return jax.vmap(jax.grad(f))(U)  # [n,3] -> (VK,Vz,Vx)\n",
    "\n",
    "def hess_V_xz(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    U = jnp.stack([K, z, x], axis=-1)\n",
    "    m_vec = jnp.asarray(m).reshape(-1)\n",
    "    def f(u):\n",
    "        return modelV(jnp.concatenate([u, m_vec]))\n",
    "    H = jax.vmap(jax.hessian(f))(U)  # [n,3,3]\n",
    "    return H[:,2,2], H[:,1,1]   # V_xx, V_zz\n",
    "\n",
    "# ------------- q-rule & payout -------------\n",
    "def invest_rate_from_q(q, cfg: Cfg):\n",
    "    s_plus = jnp.maximum((q - 1.0)/cfg.theta_plus, 0.0)\n",
    "    s_minus = jnp.minimum((q - 1.0)/cfg.theta_minus, 0.0)\n",
    "    return s_plus + s_minus\n",
    "\n",
    "def payout(K, z, x, p, s, cfg: Cfg):\n",
    "    rev = jnp.exp(x + z + p) * (K**cfg.alpha)\n",
    "    adj = 0.5 * (cfg.theta_plus*(s>=0) + cfg.theta_minus*(s<0)) * (s**2) * K\n",
    "    return rev - cfg.f - s*K - adj\n",
    "\n",
    "# ------------- Price from market clearing (reduced-form) -------------\n",
    "def price_from_panel(x, z, K, cfg: Cfg):\n",
    "    Y = jnp.mean(jnp.exp(x + z) * (K**cfg.alpha))\n",
    "    return -cfg.eta * jnp.log(clamp(Y, 1e-12, 1e12))  # log price p; set eta=0 for fixed price\n",
    "\n",
    "# ------------- SDF & risk-neutral drift -------------\n",
    "def sdf_params(C_prev, C_curr, dt, cfg: Cfg):\n",
    "    C_prev = clamp(C_prev, 1e-9)\n",
    "    C_curr = clamp(C_curr, 1e-9)\n",
    "    dlogC = jnp.log(C_curr) - jnp.log(C_prev)\n",
    "    sigma_c = jnp.abs(dlogC) / jnp.sqrt(jnp.maximum(dt, 1e-12))\n",
    "    mu_c = dlogC / jnp.maximum(dt, 1e-12) + 0.5*(sigma_c**2)\n",
    "    r_t = cfg.rho + cfg.gamma*mu_c - 0.5*cfg.gamma*(cfg.gamma+1.0)*(sigma_c**2)\n",
    "    lam_t = cfg.gamma * sigma_c\n",
    "    return r_t, lam_t\n",
    "\n",
    "def mu_x_Q(x, lam_t, cfg: Cfg):\n",
    "    return cfg.kappa_x*(cfg.mu_x - x) - cfg.sigma_x*lam_t\n",
    "\n",
    "# ------------- Simulation primitives -------------\n",
    "def sim_eps(key, shape): return jax.random.normal(key, shape)\n",
    "\n",
    "def sim_paths(key, cfg: Cfg):\n",
    "    dt = cfg.T/cfg.nT\n",
    "    nF = cfg.nF//(2 if cfg.antithetic else 1)\n",
    "    k1,k2,k3 = jax.random.split(key, 3)\n",
    "    ex = sim_eps(k1, (cfg.nT,)) * math.sqrt(dt)\n",
    "    ez = sim_eps(k2, (nF, cfg.nT)) * math.sqrt(dt)\n",
    "    if cfg.antithetic:\n",
    "        ex = jnp.concatenate([ex, -ex], axis=0)[:cfg.nT]\n",
    "        ez = jnp.concatenate([ez, -ez], axis=0)\n",
    "    K0 = jnp.full((ez.shape[0],), 1.0)\n",
    "    z0 = sim_eps(k3, (ez.shape[0],)) * 0.2\n",
    "    x0 = cfg.mu_x\n",
    "    return K0, z0, x0, ez, ex, dt\n",
    "\n",
    "# ------------- One-step generator residual (risk-neutral HJB) -------------\n",
    "def step_generator_residual(modelV, meta, K, z, x, m, m_next, cfg: Cfg):\n",
    "    p = price_from_panel(x, z, K, cfg)\n",
    "    V = V_eval(modelV, K, z, x, m)\n",
    "    g = grad_V(modelV, K, z, x, m)\n",
    "    VK, Vz, Vx = g[:,0], g[:,1], g[:,2]\n",
    "    s = invest_rate_from_q(VK, cfg)\n",
    "    pay = payout(K, z, x, p, s, cfg)\n",
    "    C_now = jnp.mean(pay)\n",
    "    C_prev = meta[\"C_prev\"]\n",
    "    r_t, lam_t = sdf_params(C_prev, C_now, meta[\"dt\"], cfg)\n",
    "\n",
    "    Vxx, Vzz = hess_V_xz(modelV, K, z, x, m)\n",
    "    mdot = (m_next - m) / jnp.maximum(meta[\"dt\"], 1e-12)\n",
    "    Vm = jax.grad(lambda M: jnp.mean(V_eval(modelV, K, z, x, M)))(m)\n",
    "    Vm_dot = jnp.dot(Vm, mdot)\n",
    "\n",
    "    # Risk-neutral generator: r*V - [flow + drifts + diffusions + distribution-derivative]\n",
    "    gen = r_t*V - (pay + VK*(s-cfg.delta)*K + Vx*mu_x_Q(x, lam_t, cfg)\n",
    "                 + Vz*(-cfg.kappa_z*z) + 0.5*(cfg.sigma_x**2)*Vxx\n",
    "                 + 0.5*(cfg.sigma_z**2)*Vzz + Vm_dot)\n",
    "\n",
    "    meta[\"C_prev\"] = C_now\n",
    "    metrics = {\n",
    "        \"Vm_norm\": jnp.linalg.norm(Vm),\n",
    "        \"p\": p,\n",
    "        \"r_t\": r_t,\n",
    "        \"lam_t\": lam_t,\n",
    "    }\n",
    "    return gen, s, p, C_now, r_t, lam_t, metrics\n",
    "\n",
    "def tamed_euler_update_K(K, s, dt, delta):\n",
    "    drift = (s - delta) * K\n",
    "    denom = 1.0 + dt * jnp.abs(s - delta)\n",
    "    return clamp(K + (dt * drift) / denom)\n",
    "\n",
    "def roll_sim_and_resid(key, modelV, deepset, cfg: Cfg):\n",
    "    K, z, x, ez, ex, dt = sim_paths(key, cfg)\n",
    "    nF, nT = ez.shape\n",
    "    m = deepset(K, z)\n",
    "    meta = {\"dt\": dt, \"C_prev\": 0.1}\n",
    "    R_list, DPR_list, r_list, lam_list, p_list = [], [], [], [], []\n",
    "    for t in range(nT):\n",
    "        z_next = z + (-cfg.kappa_z*z)*dt + cfg.sigma_z * ez[:,t]\n",
    "        x_next = x + (cfg.kappa_x*(cfg.mu_x - x))*dt + cfg.sigma_x * ex[t]\n",
    "        m_next = deepset(K, z_next)  # predictor for mdot\n",
    "        gen, s, p, C_now, r_t, lam_t, _ = step_generator_residual(\n",
    "            modelV, meta, K, z, x, m, m_next, cfg\n",
    "        )\n",
    "        R_list.append(gen)\n",
    "        V_now = clamp(V_eval(modelV, K, z, x, m), 1e-6, 1e6)\n",
    "        V_next = clamp(V_eval(modelV, K, z_next, x_next, m_next), 1e-6, 1e6)\n",
    "        DPR_list.append(jnp.mean(V_next - V_now))\n",
    "        r_list.append(r_t)\n",
    "        lam_list.append(lam_t)\n",
    "        p_list.append(p)\n",
    "        K = tamed_euler_update_K(K, s, dt, cfg.delta)\n",
    "        z, x, m = z_next, x_next, m_next\n",
    "    R = jnp.stack(R_list, axis=0)\n",
    "    DPR = jnp.stack(DPR_list, axis=0)\n",
    "    return R, DPR, jnp.stack(r_list), jnp.stack(lam_list), jnp.stack(p_list)\n",
    "\n",
    "# ---- Training & diagnostics (unchanged below) ----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe53cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[patch] shape-robust V_eval/grad_V/hess_V_xz active\n"
     ]
    }
   ],
   "source": [
    "# Patch: shape-robust V_eval/grad_V/hess_V_xz to handle scalar vs vector inputs\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "_def_msg = \"[patch] shape-robust V_eval/grad_V/hess_V_xz active\"\n",
    "print(_def_msg)\n",
    "\n",
    "def _batch3(K, z, x):\n",
    "    K = jnp.asarray(K)\n",
    "    z = jnp.asarray(z)\n",
    "    x = jnp.asarray(x)\n",
    "    if K.ndim == 0:\n",
    "        K = K[None]\n",
    "    if z.ndim == 0:\n",
    "        z = z[None]\n",
    "    # Broadcast x to match K's leading size\n",
    "    if x.shape != K.shape:\n",
    "        x = jnp.broadcast_to(x, K.shape)\n",
    "    return K, z, x\n",
    "\n",
    "def V_eval(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    m = jnp.asarray(m).reshape(-1)\n",
    "    n = K.shape[0]\n",
    "    M = jnp.broadcast_to(m, (n, m.shape[0]))\n",
    "    XYZ = jnp.stack([K, z, x], axis=-1)  # (n, 3)\n",
    "    X = jnp.concatenate([XYZ, M], axis=1)\n",
    "    # Equinox layers expect single-sample inputs; use vmap over batch\n",
    "    return jax.vmap(modelV)(X)\n",
    "\n",
    "def grad_V(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    U = jnp.stack([K, z, x], axis=-1)\n",
    "    m_vec = jnp.asarray(m).reshape(-1)\n",
    "    def f(u):\n",
    "        return modelV(jnp.concatenate([u, m_vec]))\n",
    "    return jax.vmap(jax.grad(f))(U)  # [n,3] -> grads per sample\n",
    "\n",
    "def hess_V_xz(modelV, K, z, x, m):\n",
    "    K, z, x = _batch3(K, z, x)\n",
    "    U = jnp.stack([K, z, x], axis=-1)\n",
    "    m_vec = jnp.asarray(m).reshape(-1)\n",
    "    def f(u):\n",
    "        return modelV(jnp.concatenate([u, m_vec]))\n",
    "    H = jax.vmap(jax.hessian(f))(U)  # [n,3,3]\n",
    "    return H[:,2,2], H[:,1,1]   # V_xx, V_zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca9ad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q-rule (s) = (VK - 1)/theta   (expect (VK-1)/theta)\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "sympy_checks()\n",
    "modelV, modelV_avg, deepset, logs, cfg = train(cfg, width=64, clip_norm=1.0, weight_decay=1e-4)\n",
    "print(\"Diagnostics (every ~40 steps):\")\n",
    "for row in logs[:5]:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c969c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vplot = modelV_avg   # Polyak–Ruppert / SWA-averaged params for eval\n",
    "plot_invest_rate_slices(Vplot, deepset, cfg)\n",
    "plot_residual_slice(Vplot, deepset, cfg)\n",
    "plot_policy_heatmap(Vplot, deepset, cfg)\n",
    "plot_dp_ratio(Vplot, deepset, cfg)\n",
    "plot_kink_share(Vplot, deepset, cfg, tau=0.02)\n",
    "plot_risk_params(Vplot, deepset, cfg)\n",
    "plot_price_series(Vplot, deepset, cfg)  # NEW plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e25c605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[patch] Could not access base loss function; define a minimal wrapper as fallback.\n"
     ]
    }
   ],
   "source": [
    "# Patch: ensure loss_fn uses has_aux=True\n",
    "try:\n",
    "    base_loss_fn = loss_fn.fun  # Equinox wrapper retains original callable\n",
    "except Exception:\n",
    "    try:\n",
    "        base_loss_fn = loss_fn.f\n",
    "    except Exception:\n",
    "        base_loss_fn = None\n",
    "\n",
    "if base_loss_fn is not None:\n",
    "    loss_fn = eqx.filter_value_and_grad(base_loss_fn, has_aux=True)\n",
    "    print(\"[patch] Rewrapped loss_fn with has_aux=True\")\n",
    "else:\n",
    "    print(\"[patch] Could not access base loss function; define a minimal wrapper as fallback.\")\n",
    "    def _loss_core(modelV: VNet, deepset: DeepSet, key, cfg: Cfg):\n",
    "        R, DPR, rts, lams, ps = roll_sim_and_resid(key, modelV, deepset, cfg)\n",
    "        loss_gen = jnp.mean(R**2)\n",
    "        parts = (loss_gen, jnp.mean(DPR), jnp.std(ps))\n",
    "        aux = {\n",
    "            \"tower_mean\": jnp.mean(R, axis=1).mean(),\n",
    "            \"DPR_mean\": jnp.mean(DPR),\n",
    "            \"r_abs\": jnp.mean(jnp.abs(rts)),\n",
    "            \"lam_abs\": jnp.mean(jnp.abs(lams)),\n",
    "            \"p_std\": jnp.std(ps),\n",
    "            \"kt_f\": jnp.array(0.0),\n",
    "            \"kt_b\": jnp.array(0.0),\n",
    "            \"bel\": jnp.array(0.0),\n",
    "        }\n",
    "        # minimal loss: generator residual\n",
    "        L = loss_gen\n",
    "        return L, (parts, aux)\n",
    "    loss_fn = eqx.filter_value_and_grad(_loss_core, has_aux=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Model (JAX CUDA)",
   "language": "python",
   "name": "model-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
